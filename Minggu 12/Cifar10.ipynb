{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xq_o-N2bqTK_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0i617lsxqTLA",
        "outputId": "418500ba-41d7-4c91-83d8-0da366b39a4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "felEMZhaqc_3",
        "outputId": "cf207222-8e28-4b87-9e17-5f40ba2990f8"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bVg2WvB3qTLB",
        "outputId": "5dca4765-78a0-4dfb-faa0-8bc0ca660c96"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: NVIDIA GeForce GTX 1650\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Pastikan CUDA tersedia\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda:0\")  # Pilih GPU pertama (NVIDIA)\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "oAaU3pj8qTLB"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "# Transformasi data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "# Load CIFAR10\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "# Split data\n",
        "train_size = int(0.8 * len(train_dataset))\n",
        "val_size = len(train_dataset) - train_size\n",
        "train_data, val_data = random_split(train_dataset, [train_size, val_size])\n",
        "\n",
        "# Dataloader\n",
        "batch_size = 64\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, kernel_size=3, pooling='max', num_classes=10):\n",
        "        super(CNN, self).__init__()\n",
        "        self.pooling = pooling\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=kernel_size, padding=kernel_size//2)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=kernel_size, padding=kernel_size//2)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, num_classes)\n",
        "        \n",
        "        if pooling == 'max':\n",
        "            self.pool = nn.MaxPool2d(2, 2)\n",
        "        elif pooling == 'avg':\n",
        "            self.pool = nn.AvgPool2d(2, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.optim import SGD, RMSprop, Adam\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "\n",
        "def train_model(model, optimizer, epochs, early_stop_patience, lr_scheduler, device):\n",
        "    model = model.to(device)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1) if lr_scheduler else None\n",
        "    \n",
        "    best_val_loss = float('inf')\n",
        "    patience = 0\n",
        "\n",
        "    # Untuk menyimpan riwayat training\n",
        "    history = {'train_loss': [], 'val_loss': []}\n",
        "\n",
        "    print(f\"{'Epoch':<10}{'Train Loss':<15}{'Val Loss':<15}{'Learning Rate':<15}\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        \n",
        "        train_loss /= len(train_loader)\n",
        "        history['train_loss'].append(train_loss)\n",
        "        \n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "        \n",
        "        val_loss /= len(val_loader)\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        # Save the best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= early_stop_patience:\n",
        "                print(f\"Early stopping at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        # Print progress\n",
        "        print(f\"{epoch+1:<10}{train_loss:<15.6f}{val_loss:<15.6f}{current_lr:<15.6f}\")\n",
        "    \n",
        "    return history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=SGD, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.085313       1.846693       0.010000       \n",
            "2         1.714724       1.605795       0.010000       \n",
            "3         1.532832       1.490319       0.010000       \n",
            "4         1.426621       1.391420       0.010000       \n",
            "5         1.341889       1.352678       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=SGD, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.095375       1.870570       0.010000       \n",
            "2         1.759019       1.665967       0.010000       \n",
            "3         1.561691       1.526649       0.010000       \n",
            "4         1.436652       1.425704       0.010000       \n",
            "5         1.350329       1.336158       0.010000       \n",
            "6         1.282862       1.287044       0.010000       \n",
            "7         1.225305       1.259801       0.010000       \n",
            "8         1.175173       1.211330       0.010000       \n",
            "9         1.126600       1.168030       0.010000       \n",
            "10        1.082594       1.140350       0.010000       \n",
            "11        1.040492       1.100671       0.010000       \n",
            "12        1.000615       1.112583       0.010000       \n",
            "13        0.965291       1.041500       0.010000       \n",
            "14        0.928851       1.033621       0.010000       \n",
            "15        0.894673       1.009430       0.010000       \n",
            "16        0.863875       1.013835       0.010000       \n",
            "17        0.832553       0.999983       0.010000       \n",
            "18        0.803248       0.973154       0.010000       \n",
            "19        0.773363       1.051761       0.010000       \n",
            "20        0.746610       0.991295       0.010000       \n",
            "21        0.719305       0.942377       0.010000       \n",
            "22        0.690912       0.949370       0.010000       \n",
            "23        0.664644       0.941100       0.010000       \n",
            "24        0.635728       0.938530       0.010000       \n",
            "25        0.609506       0.979173       0.010000       \n",
            "26        0.582498       0.931548       0.010000       \n",
            "27        0.556433       0.958094       0.010000       \n",
            "28        0.528440       0.974768       0.010000       \n",
            "29        0.504546       0.970652       0.010000       \n",
            "30        0.478310       0.981839       0.010000       \n",
            "31        0.377367       0.927153       0.001000       \n",
            "32        0.365891       0.931308       0.001000       \n",
            "33        0.359850       0.936164       0.001000       \n",
            "34        0.354743       0.939398       0.001000       \n",
            "35        0.350019       0.942931       0.001000       \n",
            "36        0.345966       0.944754       0.001000       \n",
            "37        0.341940       0.946630       0.001000       \n",
            "38        0.337929       0.954860       0.001000       \n",
            "39        0.333858       0.953723       0.001000       \n",
            "40        0.330343       0.958278       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=SGD, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.116430       1.876049       0.010000       \n",
            "2         1.763050       1.659125       0.010000       \n",
            "3         1.570352       1.515880       0.010000       \n",
            "4         1.435252       1.399846       0.010000       \n",
            "5         1.348062       1.353302       0.010000       \n",
            "6         1.277652       1.284849       0.010000       \n",
            "7         1.216913       1.224195       0.010000       \n",
            "8         1.165348       1.187373       0.010000       \n",
            "9         1.117366       1.150028       0.010000       \n",
            "10        1.070656       1.130426       0.010000       \n",
            "11        1.032773       1.090077       0.010000       \n",
            "12        0.992188       1.066707       0.010000       \n",
            "13        0.954337       1.044908       0.010000       \n",
            "14        0.918197       1.044435       0.010000       \n",
            "15        0.883629       1.004589       0.010000       \n",
            "16        0.850264       1.003686       0.010000       \n",
            "17        0.819398       1.013474       0.010000       \n",
            "18        0.789475       0.987857       0.010000       \n",
            "19        0.756573       0.959263       0.010000       \n",
            "20        0.727475       0.974668       0.010000       \n",
            "21        0.697411       0.975514       0.010000       \n",
            "22        0.668998       0.942587       0.010000       \n",
            "23        0.641681       0.957637       0.010000       \n",
            "24        0.613667       0.995411       0.010000       \n",
            "25        0.586720       0.984224       0.010000       \n",
            "26        0.555692       0.997605       0.010000       \n",
            "27        0.527281       0.999672       0.010000       \n",
            "28        0.497601       1.061823       0.010000       \n",
            "29        0.470536       1.015716       0.010000       \n",
            "30        0.442811       1.029004       0.010000       \n",
            "31        0.338720       0.974168       0.001000       \n",
            "Early stopping at epoch 32\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=SGD, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.085088       1.844385       0.010000       \n",
            "2         1.706922       1.604842       0.010000       \n",
            "3         1.522392       1.496900       0.010000       \n",
            "4         1.407227       1.392225       0.010000       \n",
            "5         1.322053       1.330211       0.010000       \n",
            "6         1.257092       1.275614       0.010000       \n",
            "7         1.202641       1.233233       0.010000       \n",
            "8         1.152973       1.188983       0.010000       \n",
            "9         1.110072       1.176647       0.010000       \n",
            "10        1.065709       1.125828       0.010000       \n",
            "11        1.021791       1.113709       0.010000       \n",
            "12        0.984110       1.101077       0.010000       \n",
            "13        0.947312       1.045198       0.010000       \n",
            "14        0.914008       1.037954       0.010000       \n",
            "15        0.880095       1.000213       0.010000       \n",
            "16        0.847113       1.014504       0.010000       \n",
            "17        0.816132       0.956998       0.010000       \n",
            "18        0.786012       0.939148       0.010000       \n",
            "19        0.757667       0.989594       0.010000       \n",
            "20        0.729715       0.926937       0.010000       \n",
            "21        0.696557       0.993858       0.010000       \n",
            "22        0.670161       0.922516       0.010000       \n",
            "23        0.646251       0.931101       0.010000       \n",
            "24        0.616812       0.921930       0.010000       \n",
            "25        0.588057       0.925716       0.010000       \n",
            "26        0.564169       0.935167       0.010000       \n",
            "27        0.533428       0.952869       0.010000       \n",
            "28        0.510228       0.935888       0.010000       \n",
            "29        0.484471       0.964400       0.010000       \n",
            "30        0.454286       0.996379       0.010000       \n",
            "31        0.355927       0.932869       0.001000       \n",
            "32        0.343561       0.934845       0.001000       \n",
            "33        0.337340       0.943506       0.001000       \n",
            "Early stopping at epoch 34\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=SGD, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.099924       1.862001       0.010000       \n",
            "2         1.754862       1.666069       0.010000       \n",
            "3         1.582547       1.545770       0.010000       \n",
            "4         1.451722       1.466050       0.010000       \n",
            "5         1.362854       1.352362       0.010000       \n",
            "6         1.297250       1.312320       0.010000       \n",
            "7         1.240383       1.309358       0.010000       \n",
            "8         1.192871       1.219327       0.010000       \n",
            "9         1.144610       1.182727       0.010000       \n",
            "10        1.100807       1.166261       0.010000       \n",
            "11        1.057601       1.128043       0.010000       \n",
            "12        1.017282       1.091973       0.010000       \n",
            "13        0.976288       1.077922       0.010000       \n",
            "14        0.943580       1.046509       0.010000       \n",
            "15        0.904231       1.052095       0.010000       \n",
            "16        0.869792       1.006343       0.010000       \n",
            "17        0.838848       1.002615       0.010000       \n",
            "18        0.806730       0.980686       0.010000       \n",
            "19        0.779519       1.007636       0.010000       \n",
            "20        0.746182       0.963162       0.010000       \n",
            "21        0.723170       0.945708       0.010000       \n",
            "22        0.692203       0.968438       0.010000       \n",
            "23        0.667608       0.951687       0.010000       \n",
            "24        0.635324       0.934990       0.010000       \n",
            "25        0.609921       0.937495       0.010000       \n",
            "26        0.579506       0.948781       0.010000       \n",
            "27        0.552874       0.965928       0.010000       \n",
            "28        0.527630       0.957234       0.010000       \n",
            "29        0.498362       0.995831       0.010000       \n",
            "30        0.474238       0.974445       0.010000       \n",
            "31        0.372192       0.938723       0.001000       \n",
            "32        0.359954       0.940025       0.001000       \n",
            "33        0.353658       0.947431       0.001000       \n",
            "Early stopping at epoch 34\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=RMSProp, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         6.022320       1.698429       0.010000       \n",
            "2         1.739697       1.615419       0.010000       \n",
            "3         1.451736       1.392752       0.010000       \n",
            "4         1.325333       1.340738       0.010000       \n",
            "5         1.238835       1.353685       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=RMSProp, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         10.873927      1.749677       0.010000       \n",
            "2         1.921149       1.588576       0.010000       \n",
            "3         1.558339       1.452273       0.010000       \n",
            "4         1.389449       1.386151       0.010000       \n",
            "5         1.288993       1.335465       0.010000       \n",
            "6         1.219692       1.297406       0.010000       \n",
            "7         1.152736       1.383311       0.010000       \n",
            "8         1.103786       1.298217       0.010000       \n",
            "9         1.061162       1.338092       0.010000       \n",
            "10        1.025102       1.361109       0.010000       \n",
            "11        0.983027       1.335892       0.010000       \n",
            "12        0.938816       1.396632       0.010000       \n",
            "13        0.903944       1.419973       0.010000       \n",
            "14        0.889997       1.522224       0.010000       \n",
            "15        0.863596       1.446513       0.010000       \n",
            "Early stopping at epoch 16\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=RMSProp, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         7.442944       1.737087       0.010000       \n",
            "2         1.676887       1.469413       0.010000       \n",
            "3         1.419712       1.396999       0.010000       \n",
            "4         1.307291       1.365613       0.010000       \n",
            "5         1.220649       1.282046       0.010000       \n",
            "6         1.177774       1.332421       0.010000       \n",
            "7         1.101797       1.322628       0.010000       \n",
            "8         1.043494       1.282028       0.010000       \n",
            "9         1.020622       1.345587       0.010000       \n",
            "10        0.953698       1.366828       0.010000       \n",
            "11        0.915511       1.399305       0.010000       \n",
            "12        0.889938       1.391253       0.010000       \n",
            "13        0.877229       1.538937       0.010000       \n",
            "14        0.830774       1.603710       0.010000       \n",
            "15        0.809524       1.488066       0.010000       \n",
            "16        0.787246       1.615597       0.010000       \n",
            "17        0.770957       1.653480       0.010000       \n",
            "Early stopping at epoch 18\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=RMSProp, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         5.925240       2.070809       0.010000       \n",
            "2         1.606911       1.487503       0.010000       \n",
            "3         1.425102       1.444030       0.010000       \n",
            "4         1.290224       1.343569       0.010000       \n",
            "5         1.209615       1.335745       0.010000       \n",
            "6         1.145051       1.275043       0.010000       \n",
            "7         1.100159       1.291178       0.010000       \n",
            "8         1.057409       1.326633       0.010000       \n",
            "9         1.015315       1.357833       0.010000       \n",
            "10        0.972174       1.423892       0.010000       \n",
            "11        0.938695       1.479275       0.010000       \n",
            "12        0.897517       1.471520       0.010000       \n",
            "13        0.863375       1.573206       0.010000       \n",
            "14        0.854179       1.664067       0.010000       \n",
            "15        0.814792       1.619058       0.010000       \n",
            "Early stopping at epoch 16\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=RMSProp, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         5.427977       1.626853       0.010000       \n",
            "2         1.653274       1.494300       0.010000       \n",
            "3         1.493530       1.457195       0.010000       \n",
            "4         1.325188       1.655359       0.010000       \n",
            "5         1.224334       1.319646       0.010000       \n",
            "6         1.159280       1.284926       0.010000       \n",
            "7         1.101304       1.317539       0.010000       \n",
            "8         1.049948       1.447393       0.010000       \n",
            "9         1.013482       1.362928       0.010000       \n",
            "10        0.960246       1.407537       0.010000       \n",
            "11        0.910691       1.420336       0.010000       \n",
            "12        0.884377       1.599639       0.010000       \n",
            "13        0.853900       1.507790       0.010000       \n",
            "14        0.820983       1.548675       0.010000       \n",
            "15        0.816243       1.632420       0.010000       \n",
            "Early stopping at epoch 16\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=Adam, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.881517       1.655593       0.010000       \n",
            "2         1.638199       1.643132       0.010000       \n",
            "3         1.595127       1.620468       0.010000       \n",
            "4         1.580349       1.600325       0.010000       \n",
            "5         1.561359       1.583095       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=Adam, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.845405       1.672903       0.010000       \n",
            "2         1.663839       1.632272       0.010000       \n",
            "3         1.617061       1.603817       0.010000       \n",
            "4         1.583696       1.636524       0.010000       \n",
            "5         1.561760       1.602609       0.010000       \n",
            "6         1.552280       1.636201       0.010000       \n",
            "7         1.540306       1.581304       0.010000       \n",
            "8         1.531082       1.681186       0.010000       \n",
            "9         1.524936       1.572304       0.010000       \n",
            "10        1.519097       1.568950       0.010000       \n",
            "11        1.509767       1.553773       0.010000       \n",
            "12        1.483780       1.541037       0.010000       \n",
            "13        1.462236       1.579926       0.010000       \n",
            "14        1.433711       1.509012       0.010000       \n",
            "15        1.420935       1.487732       0.010000       \n",
            "16        1.407367       1.516755       0.010000       \n",
            "17        1.403772       1.463987       0.010000       \n",
            "18        1.376666       1.486686       0.010000       \n",
            "19        1.372724       1.514676       0.010000       \n",
            "20        1.368108       1.469553       0.010000       \n",
            "21        1.353319       1.475514       0.010000       \n",
            "22        1.343436       1.470393       0.010000       \n",
            "23        1.325115       1.481772       0.010000       \n",
            "24        1.318955       1.474063       0.010000       \n",
            "25        1.309659       1.490236       0.010000       \n",
            "26        1.292679       1.509953       0.010000       \n",
            "Early stopping at epoch 27\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=Adam, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.308382       2.303859       0.010000       \n",
            "2         2.303699       2.302815       0.010000       \n",
            "3         2.303577       2.303517       0.010000       \n",
            "4         2.303636       2.303436       0.010000       \n",
            "5         2.303765       2.303405       0.010000       \n",
            "6         2.303611       2.303756       0.010000       \n",
            "7         2.303782       2.303089       0.010000       \n",
            "8         2.303348       2.303898       0.010000       \n",
            "9         2.303559       2.304037       0.010000       \n",
            "10        2.303704       2.303394       0.010000       \n",
            "11        2.303573       2.303436       0.010000       \n",
            "Early stopping at epoch 12\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=Adam, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.878740       1.801600       0.010000       \n",
            "2         1.705471       1.686981       0.010000       \n",
            "3         1.664023       1.668843       0.010000       \n",
            "4         1.636486       1.650851       0.010000       \n",
            "5         1.616577       1.624857       0.010000       \n",
            "6         1.585118       1.592050       0.010000       \n",
            "7         1.544489       1.620498       0.010000       \n",
            "8         1.511837       1.544722       0.010000       \n",
            "9         1.495716       1.509319       0.010000       \n",
            "10        1.460244       1.546786       0.010000       \n",
            "11        1.433294       1.493462       0.010000       \n",
            "12        1.416898       1.517630       0.010000       \n",
            "13        1.399947       1.522217       0.010000       \n",
            "14        1.380983       1.527384       0.010000       \n",
            "15        1.376527       1.460976       0.010000       \n",
            "16        1.366147       1.482753       0.010000       \n",
            "17        1.360967       1.473691       0.010000       \n",
            "18        1.356360       1.442170       0.010000       \n",
            "19        1.346354       1.488535       0.010000       \n",
            "20        1.346630       1.473232       0.010000       \n",
            "21        1.345182       1.503558       0.010000       \n",
            "22        1.337675       1.455319       0.010000       \n",
            "23        1.326766       1.451308       0.010000       \n",
            "24        1.333476       1.492467       0.010000       \n",
            "25        1.311058       1.522204       0.010000       \n",
            "26        1.300255       1.453465       0.010000       \n",
            "27        1.290042       1.470865       0.010000       \n",
            "Early stopping at epoch 28\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=max, Optimizer=Adam, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.759293       1.588838       0.010000       \n",
            "2         1.523924       1.546664       0.010000       \n",
            "3         1.446818       1.459767       0.010000       \n",
            "4         1.397921       1.418208       0.010000       \n",
            "5         1.354811       1.475576       0.010000       \n",
            "6         1.314382       1.359287       0.010000       \n",
            "7         1.290171       1.425362       0.010000       \n",
            "8         1.272548       1.398296       0.010000       \n",
            "9         1.248617       1.356071       0.010000       \n",
            "10        1.231192       1.363714       0.010000       \n",
            "11        1.241826       1.406242       0.010000       \n",
            "12        1.219398       1.407707       0.010000       \n",
            "13        1.215910       1.356345       0.010000       \n",
            "14        1.195956       1.357410       0.010000       \n",
            "15        1.193026       1.385089       0.010000       \n",
            "16        1.182322       1.402987       0.010000       \n",
            "17        1.188173       1.363768       0.010000       \n",
            "18        1.168492       1.360163       0.010000       \n",
            "Early stopping at epoch 19\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=SGD, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.188528       1.981846       0.010000       \n",
            "2         1.863520       1.763268       0.010000       \n",
            "3         1.698224       1.668948       0.010000       \n",
            "4         1.590570       1.561968       0.010000       \n",
            "5         1.513572       1.492083       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=SGD, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.222846       2.033741       0.010000       \n",
            "2         1.904013       1.809528       0.010000       \n",
            "3         1.753251       1.706413       0.010000       \n",
            "4         1.639416       1.615980       0.010000       \n",
            "5         1.543432       1.513282       0.010000       \n",
            "6         1.474686       1.451155       0.010000       \n",
            "7         1.423606       1.438059       0.010000       \n",
            "8         1.381122       1.446425       0.010000       \n",
            "9         1.342418       1.366631       0.010000       \n",
            "10        1.306406       1.381308       0.010000       \n",
            "11        1.268368       1.326410       0.010000       \n",
            "12        1.234886       1.273628       0.010000       \n",
            "13        1.204057       1.261330       0.010000       \n",
            "14        1.173966       1.249083       0.010000       \n",
            "15        1.147307       1.236107       0.010000       \n",
            "16        1.120486       1.211608       0.010000       \n",
            "17        1.096877       1.172868       0.010000       \n",
            "18        1.070907       1.181948       0.010000       \n",
            "19        1.046103       1.168373       0.010000       \n",
            "20        1.024943       1.137770       0.010000       \n",
            "21        1.002289       1.116877       0.010000       \n",
            "22        0.977959       1.132409       0.010000       \n",
            "23        0.957570       1.119537       0.010000       \n",
            "24        0.938639       1.116815       0.010000       \n",
            "25        0.918254       1.113144       0.010000       \n",
            "26        0.899344       1.087849       0.010000       \n",
            "27        0.877047       1.071203       0.010000       \n",
            "28        0.857114       1.136584       0.010000       \n",
            "29        0.837269       1.073927       0.010000       \n",
            "30        0.819541       1.074329       0.010000       \n",
            "31        0.742144       1.035873       0.001000       \n",
            "32        0.731947       1.036477       0.001000       \n",
            "33        0.726853       1.038290       0.001000       \n",
            "34        0.722897       1.041108       0.001000       \n",
            "35        0.719668       1.039814       0.001000       \n",
            "36        0.716355       1.040082       0.001000       \n",
            "37        0.713123       1.044370       0.001000       \n",
            "38        0.709766       1.043333       0.001000       \n",
            "39        0.706875       1.041857       0.001000       \n",
            "40        0.703661       1.046830       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=SGD, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.208868       2.030883       0.010000       \n",
            "2         1.896496       1.799474       0.010000       \n",
            "3         1.735674       1.695357       0.010000       \n",
            "4         1.637344       1.620002       0.010000       \n",
            "5         1.561791       1.539545       0.010000       \n",
            "6         1.503940       1.488551       0.010000       \n",
            "7         1.451927       1.443815       0.010000       \n",
            "8         1.405021       1.408404       0.010000       \n",
            "9         1.361977       1.391965       0.010000       \n",
            "10        1.322533       1.348570       0.010000       \n",
            "11        1.283429       1.318080       0.010000       \n",
            "12        1.246333       1.282191       0.010000       \n",
            "13        1.215831       1.261287       0.010000       \n",
            "14        1.181660       1.237302       0.010000       \n",
            "15        1.156861       1.247785       0.010000       \n",
            "16        1.130786       1.213639       0.010000       \n",
            "17        1.104066       1.182519       0.010000       \n",
            "18        1.076555       1.173786       0.010000       \n",
            "19        1.054400       1.159568       0.010000       \n",
            "20        1.029419       1.158476       0.010000       \n",
            "21        1.007765       1.136245       0.010000       \n",
            "22        0.983850       1.126790       0.010000       \n",
            "23        0.964260       1.119170       0.010000       \n",
            "24        0.942611       1.112157       0.010000       \n",
            "25        0.921441       1.108549       0.010000       \n",
            "26        0.900275       1.105269       0.010000       \n",
            "27        0.880572       1.089545       0.010000       \n",
            "28        0.858965       1.094616       0.010000       \n",
            "29        0.839233       1.073150       0.010000       \n",
            "30        0.820559       1.070540       0.010000       \n",
            "31        0.744461       1.042127       0.001000       \n",
            "32        0.735402       1.044343       0.001000       \n",
            "33        0.731091       1.041632       0.001000       \n",
            "34        0.726907       1.043561       0.001000       \n",
            "35        0.723212       1.047371       0.001000       \n",
            "36        0.720018       1.048079       0.001000       \n",
            "37        0.716955       1.046196       0.001000       \n",
            "38        0.713707       1.045452       0.001000       \n",
            "39        0.710251       1.046342       0.001000       \n",
            "40        0.707140       1.043584       0.001000       \n",
            "41        0.704296       1.048161       0.001000       \n",
            "42        0.701474       1.046964       0.001000       \n",
            "Early stopping at epoch 43\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=SGD, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.203347       2.013893       0.010000       \n",
            "2         1.876122       1.786582       0.010000       \n",
            "3         1.722285       1.684048       0.010000       \n",
            "4         1.618777       1.591532       0.010000       \n",
            "5         1.540287       1.532344       0.010000       \n",
            "6         1.477046       1.488105       0.010000       \n",
            "7         1.422263       1.436432       0.010000       \n",
            "8         1.377932       1.382603       0.010000       \n",
            "9         1.337003       1.366203       0.010000       \n",
            "10        1.301638       1.333475       0.010000       \n",
            "11        1.265475       1.300122       0.010000       \n",
            "12        1.233400       1.291450       0.010000       \n",
            "13        1.204288       1.265576       0.010000       \n",
            "14        1.174585       1.246319       0.010000       \n",
            "15        1.146689       1.218523       0.010000       \n",
            "16        1.121500       1.196807       0.010000       \n",
            "17        1.096372       1.182135       0.010000       \n",
            "18        1.070828       1.180398       0.010000       \n",
            "19        1.046772       1.159787       0.010000       \n",
            "20        1.023573       1.142379       0.010000       \n",
            "21        0.999877       1.125301       0.010000       \n",
            "22        0.976074       1.127256       0.010000       \n",
            "23        0.955342       1.141514       0.010000       \n",
            "24        0.933211       1.110112       0.010000       \n",
            "25        0.909882       1.122188       0.010000       \n",
            "26        0.891799       1.090791       0.010000       \n",
            "27        0.867944       1.089581       0.010000       \n",
            "28        0.847082       1.110470       0.010000       \n",
            "29        0.827200       1.094563       0.010000       \n",
            "30        0.807326       1.076863       0.010000       \n",
            "31        0.729220       1.047204       0.001000       \n",
            "32        0.718783       1.047626       0.001000       \n",
            "33        0.713791       1.050192       0.001000       \n",
            "34        0.709530       1.053069       0.001000       \n",
            "35        0.705884       1.052928       0.001000       \n",
            "36        0.702233       1.053766       0.001000       \n",
            "37        0.698711       1.053259       0.001000       \n",
            "38        0.695706       1.056187       0.001000       \n",
            "39        0.692209       1.054023       0.001000       \n",
            "40        0.689141       1.059426       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=SGD, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.150682       1.979612       0.010000       \n",
            "2         1.881870       1.810038       0.010000       \n",
            "3         1.738131       1.696510       0.010000       \n",
            "4         1.634689       1.599334       0.010000       \n",
            "5         1.557814       1.545097       0.010000       \n",
            "6         1.501039       1.500288       0.010000       \n",
            "7         1.452121       1.470611       0.010000       \n",
            "8         1.407469       1.445406       0.010000       \n",
            "9         1.363756       1.367762       0.010000       \n",
            "10        1.323475       1.346925       0.010000       \n",
            "11        1.282784       1.318607       0.010000       \n",
            "12        1.243955       1.291228       0.010000       \n",
            "13        1.207123       1.260079       0.010000       \n",
            "14        1.176101       1.267040       0.010000       \n",
            "15        1.147086       1.218750       0.010000       \n",
            "16        1.118161       1.227321       0.010000       \n",
            "17        1.091015       1.203474       0.010000       \n",
            "18        1.064615       1.177584       0.010000       \n",
            "19        1.042640       1.179394       0.010000       \n",
            "20        1.017194       1.132858       0.010000       \n",
            "21        0.993499       1.127176       0.010000       \n",
            "22        0.972690       1.122126       0.010000       \n",
            "23        0.950994       1.128887       0.010000       \n",
            "24        0.929853       1.103412       0.010000       \n",
            "25        0.910243       1.091871       0.010000       \n",
            "26        0.890153       1.092010       0.010000       \n",
            "27        0.866894       1.069187       0.010000       \n",
            "28        0.850149       1.084123       0.010000       \n",
            "29        0.828004       1.116957       0.010000       \n",
            "30        0.808995       1.087844       0.010000       \n",
            "31        0.730946       1.039882       0.001000       \n",
            "32        0.720363       1.043445       0.001000       \n",
            "33        0.715472       1.047236       0.001000       \n",
            "34        0.710834       1.046177       0.001000       \n",
            "35        0.707551       1.046728       0.001000       \n",
            "36        0.704267       1.046613       0.001000       \n",
            "37        0.700700       1.050915       0.001000       \n",
            "38        0.697295       1.048796       0.001000       \n",
            "39        0.694153       1.052262       0.001000       \n",
            "40        0.691047       1.051318       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=RMSProp, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         4.814677       2.470517       0.010000       \n",
            "2         1.654982       1.478061       0.010000       \n",
            "3         1.398552       1.478753       0.010000       \n",
            "4         1.263208       1.275686       0.010000       \n",
            "5         1.159084       1.264766       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=RMSProp, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         5.931175       1.564733       0.010000       \n",
            "2         1.525200       1.483522       0.010000       \n",
            "3         1.353579       1.289510       0.010000       \n",
            "4         1.244509       1.267603       0.010000       \n",
            "5         1.142601       1.181160       0.010000       \n",
            "6         1.058490       1.238865       0.010000       \n",
            "7         1.002440       1.181579       0.010000       \n",
            "8         0.937147       1.207235       0.010000       \n",
            "9         0.882674       1.256609       0.010000       \n",
            "10        0.839498       1.249718       0.010000       \n",
            "11        0.792698       1.335700       0.010000       \n",
            "12        0.759701       1.356112       0.010000       \n",
            "13        0.714558       1.407266       0.010000       \n",
            "14        0.687459       1.515168       0.010000       \n",
            "Early stopping at epoch 15\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=RMSProp, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         5.441304       1.528101       0.010000       \n",
            "2         1.614847       1.412818       0.010000       \n",
            "3         1.367267       1.374574       0.010000       \n",
            "4         1.275447       1.262722       0.010000       \n",
            "5         1.146363       1.293263       0.010000       \n",
            "6         1.078866       1.235628       0.010000       \n",
            "7         1.002426       1.338665       0.010000       \n",
            "8         0.947461       1.262353       0.010000       \n",
            "9         0.893098       1.250046       0.010000       \n",
            "10        0.835536       1.323729       0.010000       \n",
            "11        0.794804       1.333368       0.010000       \n",
            "12        0.758169       1.384558       0.010000       \n",
            "13        0.707478       1.385711       0.010000       \n",
            "14        0.668250       1.567679       0.010000       \n",
            "15        0.633781       1.708379       0.010000       \n",
            "Early stopping at epoch 16\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=RMSProp, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         5.608630       1.564752       0.010000       \n",
            "2         1.656235       1.439917       0.010000       \n",
            "3         1.447760       1.298154       0.010000       \n",
            "4         1.281811       1.260615       0.010000       \n",
            "5         1.133174       1.192185       0.010000       \n",
            "6         1.042231       1.207404       0.010000       \n",
            "7         0.964638       1.262754       0.010000       \n",
            "8         0.904878       1.169560       0.010000       \n",
            "9         0.852416       1.241918       0.010000       \n",
            "10        0.797067       1.255535       0.010000       \n",
            "11        0.755818       1.299389       0.010000       \n",
            "12        0.715993       1.365919       0.010000       \n",
            "13        0.681524       1.440107       0.010000       \n",
            "14        0.649455       1.482884       0.010000       \n",
            "15        0.613231       1.573629       0.010000       \n",
            "16        0.586309       1.630863       0.010000       \n",
            "17        0.560653       1.789663       0.010000       \n",
            "Early stopping at epoch 18\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=RMSProp, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         5.845792       1.749304       0.010000       \n",
            "2         1.684527       1.491428       0.010000       \n",
            "3         1.446856       1.384367       0.010000       \n",
            "4         1.319434       1.389453       0.010000       \n",
            "5         1.186482       1.268597       0.010000       \n",
            "6         1.090613       1.275172       0.010000       \n",
            "7         1.020846       1.265360       0.010000       \n",
            "8         0.940862       1.243199       0.010000       \n",
            "9         0.909685       1.269197       0.010000       \n",
            "10        0.815937       1.391775       0.010000       \n",
            "11        0.762004       1.393334       0.010000       \n",
            "12        0.716001       1.469516       0.010000       \n",
            "13        0.670719       1.562166       0.010000       \n",
            "14        0.627351       1.671957       0.010000       \n",
            "15        0.598505       1.697933       0.010000       \n",
            "16        0.591270       1.815731       0.010000       \n",
            "17        0.536959       1.891206       0.010000       \n",
            "Early stopping at epoch 18\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=Adam, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.688939       1.476470       0.010000       \n",
            "2         1.420423       1.436830       0.010000       \n",
            "3         1.328838       1.336163       0.010000       \n",
            "4         1.270761       1.331833       0.010000       \n",
            "5         1.227288       1.322468       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=Adam, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.962619       1.737581       0.010000       \n",
            "2         1.669280       1.679467       0.010000       \n",
            "3         1.610248       1.633253       0.010000       \n",
            "4         1.571386       1.592771       0.010000       \n",
            "5         1.546825       1.569695       0.010000       \n",
            "6         1.526413       1.568902       0.010000       \n",
            "7         1.509634       1.543828       0.010000       \n",
            "8         1.495012       1.539420       0.010000       \n",
            "9         1.478654       1.539125       0.010000       \n",
            "10        1.472272       1.551407       0.010000       \n",
            "11        1.463353       1.563131       0.010000       \n",
            "12        1.462255       1.525757       0.010000       \n",
            "13        1.458695       1.579934       0.010000       \n",
            "14        1.451439       1.551215       0.010000       \n",
            "15        1.443005       1.527901       0.010000       \n",
            "16        1.440213       1.524360       0.010000       \n",
            "17        1.438643       1.528588       0.010000       \n",
            "18        1.431164       1.532083       0.010000       \n",
            "19        1.431392       1.539440       0.010000       \n",
            "20        1.425083       1.505573       0.010000       \n",
            "21        1.417807       1.496821       0.010000       \n",
            "22        1.401925       1.511867       0.010000       \n",
            "23        1.394495       1.504617       0.010000       \n",
            "24        1.375745       1.485472       0.010000       \n",
            "25        1.379367       1.486084       0.010000       \n",
            "26        1.356405       1.481610       0.010000       \n",
            "27        1.342356       1.525266       0.010000       \n",
            "28        1.333992       1.537304       0.010000       \n",
            "29        1.336145       1.522670       0.010000       \n",
            "30        1.320560       1.498083       0.010000       \n",
            "31        1.217864       1.440635       0.001000       \n",
            "32        1.196810       1.434702       0.001000       \n",
            "33        1.189376       1.438870       0.001000       \n",
            "34        1.184676       1.441961       0.001000       \n",
            "35        1.181296       1.434278       0.001000       \n",
            "36        1.178021       1.438204       0.001000       \n",
            "37        1.174137       1.436342       0.001000       \n",
            "38        1.171814       1.432855       0.001000       \n",
            "39        1.169124       1.446942       0.001000       \n",
            "40        1.165903       1.440894       0.001000       \n",
            "41        1.164520       1.449664       0.001000       \n",
            "42        1.162584       1.446003       0.001000       \n",
            "43        1.160125       1.450015       0.001000       \n",
            "44        1.156966       1.436157       0.001000       \n",
            "45        1.156077       1.446204       0.001000       \n",
            "46        1.153605       1.446087       0.001000       \n",
            "47        1.151301       1.456365       0.001000       \n",
            "Early stopping at epoch 48\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=Adam, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.306598       2.302829       0.010000       \n",
            "2         2.303395       2.303288       0.010000       \n",
            "3         2.303616       2.303371       0.010000       \n",
            "4         2.303303       2.305467       0.010000       \n",
            "5         2.303664       2.302896       0.010000       \n",
            "6         2.303533       2.305359       0.010000       \n",
            "7         2.303563       2.303927       0.010000       \n",
            "8         2.303551       2.303799       0.010000       \n",
            "9         2.303596       2.304814       0.010000       \n",
            "10        2.303393       2.302970       0.010000       \n",
            "11        2.303488       2.302654       0.010000       \n",
            "12        2.303552       2.303265       0.010000       \n",
            "13        2.303711       2.302524       0.010000       \n",
            "14        2.303592       2.302940       0.010000       \n",
            "15        2.303558       2.302931       0.010000       \n",
            "16        2.303453       2.302999       0.010000       \n",
            "17        2.303739       2.303474       0.010000       \n",
            "18        2.303622       2.303096       0.010000       \n",
            "19        2.303674       2.303503       0.010000       \n",
            "20        2.303604       2.305804       0.010000       \n",
            "21        2.303792       2.303682       0.010000       \n",
            "22        2.303435       2.303940       0.010000       \n",
            "Early stopping at epoch 23\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=Adam, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.643407       1.543255       0.010000       \n",
            "2         1.392703       1.374149       0.010000       \n",
            "3         1.306628       1.302983       0.010000       \n",
            "4         1.242525       1.265197       0.010000       \n",
            "5         1.195213       1.292398       0.010000       \n",
            "6         1.168499       1.262422       0.010000       \n",
            "7         1.143105       1.296627       0.010000       \n",
            "8         1.114064       1.234753       0.010000       \n",
            "9         1.090121       1.273215       0.010000       \n",
            "10        1.072089       1.240809       0.010000       \n",
            "11        1.045644       1.267868       0.010000       \n",
            "12        1.026479       1.309519       0.010000       \n",
            "13        1.011072       1.348824       0.010000       \n",
            "14        0.993274       1.321612       0.010000       \n",
            "15        0.968582       1.335318       0.010000       \n",
            "16        0.958653       1.331628       0.010000       \n",
            "17        0.946098       1.360859       0.010000       \n",
            "Early stopping at epoch 18\n",
            "\n",
            "Running experiment: Kernel Size=3, Pooling=avg, Optimizer=Adam, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.838085       1.700529       0.010000       \n",
            "2         1.626285       1.622290       0.010000       \n",
            "3         1.568341       1.572061       0.010000       \n",
            "4         1.526709       1.591387       0.010000       \n",
            "5         1.505838       1.549375       0.010000       \n",
            "6         1.487182       1.546794       0.010000       \n",
            "7         1.470054       1.536058       0.010000       \n",
            "8         1.461654       1.537980       0.010000       \n",
            "9         1.441660       1.513260       0.010000       \n",
            "10        1.412289       1.508537       0.010000       \n",
            "11        1.385002       1.467321       0.010000       \n",
            "12        1.359228       1.486471       0.010000       \n",
            "13        1.343346       1.476414       0.010000       \n",
            "14        1.331175       1.463323       0.010000       \n",
            "15        1.317790       1.476420       0.010000       \n",
            "16        1.307677       1.552580       0.010000       \n",
            "17        1.294778       1.477553       0.010000       \n",
            "18        1.285653       1.447267       0.010000       \n",
            "19        1.275989       1.464300       0.010000       \n",
            "20        1.265558       1.456386       0.010000       \n",
            "21        1.261160       1.440692       0.010000       \n",
            "22        1.247342       1.464676       0.010000       \n",
            "23        1.249463       1.469835       0.010000       \n",
            "24        1.235645       1.471068       0.010000       \n",
            "25        1.239329       1.498698       0.010000       \n",
            "26        1.230058       1.496807       0.010000       \n",
            "27        1.230957       1.492852       0.010000       \n",
            "28        1.223616       1.464558       0.010000       \n",
            "29        1.216233       1.461593       0.010000       \n",
            "30        1.216497       1.525720       0.010000       \n",
            "31        1.090977       1.438370       0.001000       \n",
            "32        1.063629       1.440648       0.001000       \n",
            "33        1.054225       1.438401       0.001000       \n",
            "34        1.047253       1.442428       0.001000       \n",
            "35        1.041231       1.448059       0.001000       \n",
            "36        1.037478       1.446803       0.001000       \n",
            "37        1.032567       1.457879       0.001000       \n",
            "38        1.029287       1.456804       0.001000       \n",
            "39        1.025026       1.461600       0.001000       \n",
            "40        1.020326       1.466447       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=SGD, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.074838       1.830375       0.010000       \n",
            "2         1.695845       1.574166       0.010000       \n",
            "3         1.497771       1.458753       0.010000       \n",
            "4         1.389502       1.358936       0.010000       \n",
            "5         1.308300       1.295681       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=SGD, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.034169       1.808102       0.010000       \n",
            "2         1.649643       1.560355       0.010000       \n",
            "3         1.472041       1.465601       0.010000       \n",
            "4         1.369662       1.361831       0.010000       \n",
            "5         1.289020       1.286651       0.010000       \n",
            "6         1.215449       1.237446       0.010000       \n",
            "7         1.148673       1.166743       0.010000       \n",
            "8         1.087606       1.124067       0.010000       \n",
            "9         1.035041       1.087515       0.010000       \n",
            "10        0.985880       1.054247       0.010000       \n",
            "11        0.938706       1.031257       0.010000       \n",
            "12        0.893161       0.990073       0.010000       \n",
            "13        0.855592       0.963445       0.010000       \n",
            "14        0.819819       0.945180       0.010000       \n",
            "15        0.783939       0.970342       0.010000       \n",
            "16        0.751441       0.953101       0.010000       \n",
            "17        0.720113       0.909083       0.010000       \n",
            "18        0.685055       0.911010       0.010000       \n",
            "19        0.653436       0.893036       0.010000       \n",
            "20        0.622211       0.925511       0.010000       \n",
            "21        0.591361       0.948701       0.010000       \n",
            "22        0.561126       0.910469       0.010000       \n",
            "23        0.529040       0.892918       0.010000       \n",
            "24        0.496842       0.959226       0.010000       \n",
            "25        0.467651       0.908205       0.010000       \n",
            "26        0.438682       0.950465       0.010000       \n",
            "27        0.407260       0.929273       0.010000       \n",
            "28        0.380529       0.966695       0.010000       \n",
            "29        0.349167       0.978358       0.010000       \n",
            "30        0.318676       1.031329       0.010000       \n",
            "31        0.223329       0.954793       0.001000       \n",
            "32        0.209555       0.965277       0.001000       \n",
            "Early stopping at epoch 33\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=SGD, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.059305       1.816999       0.010000       \n",
            "2         1.659976       1.569482       0.010000       \n",
            "3         1.484326       1.452061       0.010000       \n",
            "4         1.383256       1.355319       0.010000       \n",
            "5         1.301763       1.320888       0.010000       \n",
            "6         1.227853       1.250065       0.010000       \n",
            "7         1.164793       1.193614       0.010000       \n",
            "8         1.110461       1.134100       0.010000       \n",
            "9         1.057941       1.110267       0.010000       \n",
            "10        1.011680       1.074679       0.010000       \n",
            "11        0.967243       1.047545       0.010000       \n",
            "12        0.929233       1.038465       0.010000       \n",
            "13        0.889431       1.016600       0.010000       \n",
            "14        0.851714       0.977028       0.010000       \n",
            "15        0.816004       0.949508       0.010000       \n",
            "16        0.781947       0.973641       0.010000       \n",
            "17        0.748274       0.940250       0.010000       \n",
            "18        0.713799       0.916746       0.010000       \n",
            "19        0.685069       0.902975       0.010000       \n",
            "20        0.653300       0.897502       0.010000       \n",
            "21        0.617708       0.927385       0.010000       \n",
            "22        0.587330       0.937010       0.010000       \n",
            "23        0.560739       0.895590       0.010000       \n",
            "24        0.523643       0.921047       0.010000       \n",
            "25        0.494357       0.933739       0.010000       \n",
            "26        0.463855       0.917439       0.010000       \n",
            "27        0.432909       0.926733       0.010000       \n",
            "28        0.400558       0.935485       0.010000       \n",
            "29        0.372361       0.985248       0.010000       \n",
            "30        0.341398       1.015438       0.010000       \n",
            "31        0.241238       0.966450       0.001000       \n",
            "32        0.226783       0.970718       0.001000       \n",
            "Early stopping at epoch 33\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=SGD, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.060455       1.862104       0.010000       \n",
            "2         1.711734       1.605343       0.010000       \n",
            "3         1.499037       1.475832       0.010000       \n",
            "4         1.382386       1.379402       0.010000       \n",
            "5         1.303723       1.355871       0.010000       \n",
            "6         1.235266       1.239284       0.010000       \n",
            "7         1.174743       1.191674       0.010000       \n",
            "8         1.111972       1.134585       0.010000       \n",
            "9         1.056183       1.150525       0.010000       \n",
            "10        1.003920       1.046881       0.010000       \n",
            "11        0.953842       1.025063       0.010000       \n",
            "12        0.912569       0.999468       0.010000       \n",
            "13        0.872454       0.984885       0.010000       \n",
            "14        0.835138       0.952025       0.010000       \n",
            "15        0.801810       0.960009       0.010000       \n",
            "16        0.766875       0.930176       0.010000       \n",
            "17        0.732851       0.925705       0.010000       \n",
            "18        0.701371       0.916536       0.010000       \n",
            "19        0.669808       0.918160       0.010000       \n",
            "20        0.638662       0.951657       0.010000       \n",
            "21        0.604492       0.918610       0.010000       \n",
            "22        0.574993       0.916832       0.010000       \n",
            "23        0.542715       0.924392       0.010000       \n",
            "24        0.513681       0.929394       0.010000       \n",
            "25        0.481882       0.906043       0.010000       \n",
            "26        0.451272       0.938786       0.010000       \n",
            "27        0.419358       0.937621       0.010000       \n",
            "28        0.392464       0.961229       0.010000       \n",
            "29        0.362900       0.963077       0.010000       \n",
            "30        0.332607       1.071391       0.010000       \n",
            "31        0.232023       0.958031       0.001000       \n",
            "32        0.218001       0.967777       0.001000       \n",
            "33        0.211740       0.972611       0.001000       \n",
            "34        0.206807       0.979262       0.001000       \n",
            "Early stopping at epoch 35\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=SGD, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.088075       1.828075       0.010000       \n",
            "2         1.686059       1.576876       0.010000       \n",
            "3         1.489795       1.432651       0.010000       \n",
            "4         1.384812       1.358829       0.010000       \n",
            "5         1.307725       1.310505       0.010000       \n",
            "6         1.239092       1.320304       0.010000       \n",
            "7         1.179473       1.181197       0.010000       \n",
            "8         1.119790       1.149127       0.010000       \n",
            "9         1.066457       1.113991       0.010000       \n",
            "10        1.017688       1.090733       0.010000       \n",
            "11        0.971434       1.046606       0.010000       \n",
            "12        0.926636       1.043543       0.010000       \n",
            "13        0.885637       0.978428       0.010000       \n",
            "14        0.848991       0.991089       0.010000       \n",
            "15        0.812099       0.969883       0.010000       \n",
            "16        0.777012       0.940328       0.010000       \n",
            "17        0.742608       0.911107       0.010000       \n",
            "18        0.711095       0.935766       0.010000       \n",
            "19        0.675663       0.934926       0.010000       \n",
            "20        0.643411       0.903518       0.010000       \n",
            "21        0.610920       0.884802       0.010000       \n",
            "22        0.580794       0.898648       0.010000       \n",
            "23        0.547395       0.928431       0.010000       \n",
            "24        0.516745       0.930088       0.010000       \n",
            "25        0.490148       0.912317       0.010000       \n",
            "26        0.456742       0.938077       0.010000       \n",
            "27        0.428370       0.943642       0.010000       \n",
            "28        0.398224       0.934712       0.010000       \n",
            "29        0.368300       0.950071       0.010000       \n",
            "30        0.338802       0.990026       0.010000       \n",
            "Early stopping at epoch 31\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=RMSProp, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         44.031589      1.990035       0.010000       \n",
            "2         2.000555       1.987963       0.010000       \n",
            "3         1.927298       1.937011       0.010000       \n",
            "4         2.047134       2.295728       0.010000       \n",
            "5         1.926234       1.722075       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=RMSProp, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         32.548658      2.142940       0.010000       \n",
            "2         2.253096       2.011872       0.010000       \n",
            "3         1.952648       1.792029       0.010000       \n",
            "4         1.777699       1.655883       0.010000       \n",
            "5         1.687481       1.631766       0.010000       \n",
            "6         1.581664       1.558120       0.010000       \n",
            "7         1.516172       1.486122       0.010000       \n",
            "8         1.460267       1.461331       0.010000       \n",
            "9         1.450338       1.476899       0.010000       \n",
            "10        1.395854       1.882534       0.010000       \n",
            "11        1.587566       1.472835       0.010000       \n",
            "12        1.360230       1.512707       0.010000       \n",
            "13        1.322787       1.520129       0.010000       \n",
            "14        1.373774       1.547872       0.010000       \n",
            "15        1.324493       1.486576       0.010000       \n",
            "16        1.271859       1.562825       0.010000       \n",
            "17        1.267490       1.587092       0.010000       \n",
            "Early stopping at epoch 18\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=RMSProp, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         37.719474      1.932820       0.010000       \n",
            "2         2.095590       2.094271       0.010000       \n",
            "3         2.026951       1.877534       0.010000       \n",
            "4         1.874534       1.735468       0.010000       \n",
            "5         1.752977       1.705593       0.010000       \n",
            "6         1.599624       1.544397       0.010000       \n",
            "7         1.555452       1.534635       0.010000       \n",
            "8         1.485309       1.559770       0.010000       \n",
            "9         1.434393       1.572099       0.010000       \n",
            "10        1.391159       1.474204       0.010000       \n",
            "11        1.380771       1.584152       0.010000       \n",
            "12        1.339242       1.464264       0.010000       \n",
            "13        1.368334       1.611215       0.010000       \n",
            "14        1.327157       1.483389       0.010000       \n",
            "15        1.410479       1.463830       0.010000       \n",
            "16        1.270247       1.484333       0.010000       \n",
            "17        1.253538       1.538546       0.010000       \n",
            "18        1.232991       1.645548       0.010000       \n",
            "19        1.269390       1.535124       0.010000       \n",
            "20        2.345402       1.978264       0.010000       \n",
            "21        1.365695       1.630554       0.010000       \n",
            "22        1.251905       2.106171       0.010000       \n",
            "23        1.272089       1.603846       0.010000       \n",
            "24        1.214248       1.621907       0.010000       \n",
            "Early stopping at epoch 25\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=RMSProp, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         37.531924      1.956473       0.010000       \n",
            "2         4.699853       2.069871       0.010000       \n",
            "3         2.047133       1.965786       0.010000       \n",
            "4         1.941683       1.870252       0.010000       \n",
            "5         1.920483       1.846924       0.010000       \n",
            "6         1.899107       1.821411       0.010000       \n",
            "7         1.754055       1.799665       0.010000       \n",
            "8         1.990094       1.645727       0.010000       \n",
            "9         1.669462       1.644970       0.010000       \n",
            "10        1.626540       1.603930       0.010000       \n",
            "11        1.538809       1.587773       0.010000       \n",
            "12        1.522145       1.520142       0.010000       \n",
            "13        1.478269       1.495900       0.010000       \n",
            "14        1.438888       1.617019       0.010000       \n",
            "15        1.397728       1.447878       0.010000       \n",
            "16        1.380815       1.463091       0.010000       \n",
            "17        1.416696       1.468633       0.010000       \n",
            "18        1.359450       1.412448       0.010000       \n",
            "19        1.312633       1.419487       0.010000       \n",
            "20        1.315659       1.474006       0.010000       \n",
            "21        1.393918       1.415934       0.010000       \n",
            "22        1.270577       1.500893       0.010000       \n",
            "23        1.260595       1.668169       0.010000       \n",
            "24        1.227631       1.458631       0.010000       \n",
            "25        1.225603       1.483458       0.010000       \n",
            "26        1.603128       1.636897       0.010000       \n",
            "27        1.270625       1.606250       0.010000       \n",
            "Early stopping at epoch 28\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=RMSProp, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         29.417889      1.938493       0.010000       \n",
            "2         2.788921       2.108549       0.010000       \n",
            "3         2.119130       1.923098       0.010000       \n",
            "4         1.882576       1.856026       0.010000       \n",
            "5         1.797232       1.729200       0.010000       \n",
            "6         1.724398       1.640468       0.010000       \n",
            "7         1.655884       1.575603       0.010000       \n",
            "8         2.017492       2.291030       0.010000       \n",
            "9         1.861809       1.663974       0.010000       \n",
            "10        1.551638       1.570761       0.010000       \n",
            "11        1.487702       1.601138       0.010000       \n",
            "12        1.461620       1.483131       0.010000       \n",
            "13        1.789785       1.971006       0.010000       \n",
            "14        1.471180       1.453461       0.010000       \n",
            "15        1.437590       1.476486       0.010000       \n",
            "16        1.341892       1.512847       0.010000       \n",
            "17        1.309971       1.433226       0.010000       \n",
            "18        1.280950       1.482220       0.010000       \n",
            "19        1.347460       1.411557       0.010000       \n",
            "20        1.245982       1.452086       0.010000       \n",
            "21        1.237673       1.550340       0.010000       \n",
            "22        1.237336       1.534949       0.010000       \n",
            "23        1.212667       1.578415       0.010000       \n",
            "24        1.186384       1.571665       0.010000       \n",
            "25        1.171867       1.708355       0.010000       \n",
            "26        1.167437       1.538640       0.010000       \n",
            "27        1.161183       1.552006       0.010000       \n",
            "28        1.145164       1.566720       0.010000       \n",
            "Early stopping at epoch 29\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=Adam, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.971292       1.771027       0.010000       \n",
            "2         1.725455       1.639146       0.010000       \n",
            "3         1.657715       1.638236       0.010000       \n",
            "4         1.590570       1.625583       0.010000       \n",
            "5         1.567704       1.563937       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=Adam, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.967880       1.857489       0.010000       \n",
            "2         1.759493       1.706256       0.010000       \n",
            "3         1.695362       1.686563       0.010000       \n",
            "4         1.646677       1.647822       0.010000       \n",
            "5         1.607210       1.596515       0.010000       \n",
            "6         1.584507       1.662259       0.010000       \n",
            "7         1.660943       1.793407       0.010000       \n",
            "8         1.690381       1.749789       0.010000       \n",
            "9         1.598876       1.628868       0.010000       \n",
            "10        1.534050       1.545734       0.010000       \n",
            "11        1.501836       1.536278       0.010000       \n",
            "12        1.479566       1.480531       0.010000       \n",
            "13        1.451646       1.475205       0.010000       \n",
            "14        1.451316       1.494783       0.010000       \n",
            "15        1.432203       1.494673       0.010000       \n",
            "16        1.398707       1.499597       0.010000       \n",
            "17        1.380507       1.425603       0.010000       \n",
            "18        1.379773       1.469870       0.010000       \n",
            "19        1.360446       1.463633       0.010000       \n",
            "20        1.343299       1.488495       0.010000       \n",
            "21        1.341869       1.490988       0.010000       \n",
            "22        1.322605       1.481001       0.010000       \n",
            "23        1.331297       1.490444       0.010000       \n",
            "24        1.301790       1.427410       0.010000       \n",
            "25        1.312606       1.455376       0.010000       \n",
            "26        1.302475       1.489105       0.010000       \n",
            "Early stopping at epoch 27\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=Adam, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.364057       2.303665       0.010000       \n",
            "2         2.303585       2.303806       0.010000       \n",
            "3         2.303684       2.303599       0.010000       \n",
            "4         2.303622       2.304461       0.010000       \n",
            "5         2.303666       2.304758       0.010000       \n",
            "6         2.303551       2.302823       0.010000       \n",
            "7         2.303560       2.303659       0.010000       \n",
            "8         2.303732       2.303967       0.010000       \n",
            "9         2.303626       2.304044       0.010000       \n",
            "10        2.303605       2.302821       0.010000       \n",
            "11        2.303601       2.304150       0.010000       \n",
            "12        2.303662       2.303481       0.010000       \n",
            "13        2.303466       2.305715       0.010000       \n",
            "14        2.303795       2.303990       0.010000       \n",
            "15        2.303623       2.303977       0.010000       \n",
            "16        2.303670       2.303858       0.010000       \n",
            "17        2.303492       2.304512       0.010000       \n",
            "18        2.303296       2.303039       0.010000       \n",
            "19        2.303674       2.303155       0.010000       \n",
            "Early stopping at epoch 20\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=Adam, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.887228       1.683552       0.010000       \n",
            "2         1.649491       1.639939       0.010000       \n",
            "3         1.579299       1.608500       0.010000       \n",
            "4         1.522428       1.603638       0.010000       \n",
            "5         1.495484       1.582111       0.010000       \n",
            "6         1.461672       1.486793       0.010000       \n",
            "7         1.460452       1.561169       0.010000       \n",
            "8         1.413852       1.473276       0.010000       \n",
            "9         1.390796       1.609679       0.010000       \n",
            "10        1.380920       1.519431       0.010000       \n",
            "11        1.358018       1.504122       0.010000       \n",
            "12        1.352499       1.484901       0.010000       \n",
            "13        1.341675       1.541052       0.010000       \n",
            "14        1.346357       1.516060       0.010000       \n",
            "15        1.316774       1.568200       0.010000       \n",
            "16        1.287074       1.555306       0.010000       \n",
            "17        1.287147       1.579071       0.010000       \n",
            "Early stopping at epoch 18\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=max, Optimizer=Adam, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.987399       1.812401       0.010000       \n",
            "2         1.701484       1.622587       0.010000       \n",
            "3         1.593268       1.577417       0.010000       \n",
            "4         1.536477       1.561990       0.010000       \n",
            "5         1.495277       1.581711       0.010000       \n",
            "6         1.453296       1.529675       0.010000       \n",
            "7         1.428129       1.547829       0.010000       \n",
            "8         1.399330       1.520593       0.010000       \n",
            "9         1.373691       1.531301       0.010000       \n",
            "10        1.362296       1.500014       0.010000       \n",
            "11        1.341132       1.504492       0.010000       \n",
            "12        1.319825       1.521033       0.010000       \n",
            "13        1.320466       1.541475       0.010000       \n",
            "14        1.289045       1.484659       0.010000       \n",
            "15        1.281310       1.533674       0.010000       \n",
            "16        1.274730       1.515233       0.010000       \n",
            "17        1.255160       1.557015       0.010000       \n",
            "18        1.263597       1.562891       0.010000       \n",
            "19        1.271422       1.563299       0.010000       \n",
            "20        1.238403       1.569819       0.010000       \n",
            "21        1.215909       1.643336       0.010000       \n",
            "22        1.216107       1.544437       0.010000       \n",
            "23        1.202931       1.565463       0.010000       \n",
            "Early stopping at epoch 24\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=SGD, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.159091       1.916981       0.010000       \n",
            "2         1.800059       1.704903       0.010000       \n",
            "3         1.637145       1.593807       0.010000       \n",
            "4         1.536207       1.505783       0.010000       \n",
            "5         1.449725       1.423204       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=SGD, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.109607       1.896479       0.010000       \n",
            "2         1.791947       1.711267       0.010000       \n",
            "3         1.631826       1.583135       0.010000       \n",
            "4         1.524455       1.486719       0.010000       \n",
            "5         1.441106       1.438262       0.010000       \n",
            "6         1.377538       1.373588       0.010000       \n",
            "7         1.327154       1.338879       0.010000       \n",
            "8         1.286362       1.311168       0.010000       \n",
            "9         1.247887       1.275568       0.010000       \n",
            "10        1.212205       1.244228       0.010000       \n",
            "11        1.177102       1.219550       0.010000       \n",
            "12        1.143522       1.202728       0.010000       \n",
            "13        1.110772       1.196869       0.010000       \n",
            "14        1.079389       1.158233       0.010000       \n",
            "15        1.048928       1.138165       0.010000       \n",
            "16        1.019536       1.115121       0.010000       \n",
            "17        0.989601       1.121591       0.010000       \n",
            "18        0.961999       1.096075       0.010000       \n",
            "19        0.931860       1.079958       0.010000       \n",
            "20        0.904033       1.107575       0.010000       \n",
            "21        0.878041       1.101207       0.010000       \n",
            "22        0.849337       1.065351       0.010000       \n",
            "23        0.823611       1.029093       0.010000       \n",
            "24        0.796815       1.020476       0.010000       \n",
            "25        0.770527       1.018934       0.010000       \n",
            "26        0.744309       1.023648       0.010000       \n",
            "27        0.717336       1.023339       0.010000       \n",
            "28        0.693327       1.030682       0.010000       \n",
            "29        0.667292       1.019208       0.010000       \n",
            "30        0.643054       1.027942       0.010000       \n",
            "31        0.546805       0.991183       0.001000       \n",
            "32        0.533713       0.992175       0.001000       \n",
            "33        0.527666       0.998053       0.001000       \n",
            "34        0.522164       1.000393       0.001000       \n",
            "35        0.517407       1.004982       0.001000       \n",
            "36        0.512918       1.009543       0.001000       \n",
            "37        0.508875       1.013688       0.001000       \n",
            "38        0.504896       1.009933       0.001000       \n",
            "39        0.500628       1.011176       0.001000       \n",
            "40        0.496650       1.013038       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=SGD, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.168567       1.939833       0.010000       \n",
            "2         1.821861       1.721755       0.010000       \n",
            "3         1.654383       1.622415       0.010000       \n",
            "4         1.544129       1.516448       0.010000       \n",
            "5         1.456805       1.459171       0.010000       \n",
            "6         1.394143       1.387076       0.010000       \n",
            "7         1.344599       1.353796       0.010000       \n",
            "8         1.300557       1.326858       0.010000       \n",
            "9         1.261593       1.286865       0.010000       \n",
            "10        1.225039       1.275598       0.010000       \n",
            "11        1.191516       1.268802       0.010000       \n",
            "12        1.155987       1.271912       0.010000       \n",
            "13        1.123082       1.199680       0.010000       \n",
            "14        1.092127       1.184960       0.010000       \n",
            "15        1.063518       1.149973       0.010000       \n",
            "16        1.031418       1.148077       0.010000       \n",
            "17        1.001261       1.134239       0.010000       \n",
            "18        0.972293       1.136879       0.010000       \n",
            "19        0.944079       1.108076       0.010000       \n",
            "20        0.916195       1.111154       0.010000       \n",
            "21        0.887306       1.072959       0.010000       \n",
            "22        0.861539       1.072377       0.010000       \n",
            "23        0.834120       1.069435       0.010000       \n",
            "24        0.806809       1.059764       0.010000       \n",
            "25        0.779867       1.054080       0.010000       \n",
            "26        0.755043       1.051935       0.010000       \n",
            "27        0.729281       1.041089       0.010000       \n",
            "28        0.703399       1.037752       0.010000       \n",
            "29        0.678067       1.050062       0.010000       \n",
            "30        0.653113       1.077523       0.010000       \n",
            "31        0.557658       1.005655       0.001000       \n",
            "32        0.542720       1.010729       0.001000       \n",
            "33        0.536315       1.016408       0.001000       \n",
            "34        0.531097       1.019911       0.001000       \n",
            "35        0.526504       1.022229       0.001000       \n",
            "36        0.522196       1.024993       0.001000       \n",
            "37        0.517636       1.026890       0.001000       \n",
            "38        0.513529       1.028753       0.001000       \n",
            "39        0.509930       1.028115       0.001000       \n",
            "40        0.505548       1.031172       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=SGD, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.131943       1.912024       0.010000       \n",
            "2         1.806860       1.703602       0.010000       \n",
            "3         1.631311       1.604436       0.010000       \n",
            "4         1.522343       1.496256       0.010000       \n",
            "5         1.446475       1.457345       0.010000       \n",
            "6         1.385857       1.401312       0.010000       \n",
            "7         1.335926       1.351914       0.010000       \n",
            "8         1.292936       1.324692       0.010000       \n",
            "9         1.255239       1.299349       0.010000       \n",
            "10        1.219583       1.286068       0.010000       \n",
            "11        1.187003       1.235729       0.010000       \n",
            "12        1.152499       1.223252       0.010000       \n",
            "13        1.118365       1.192455       0.010000       \n",
            "14        1.086314       1.167512       0.010000       \n",
            "15        1.055109       1.173396       0.010000       \n",
            "16        1.024429       1.138170       0.010000       \n",
            "17        0.993468       1.141229       0.010000       \n",
            "18        0.963503       1.095378       0.010000       \n",
            "19        0.932895       1.099859       0.010000       \n",
            "20        0.904577       1.068506       0.010000       \n",
            "21        0.875957       1.047955       0.010000       \n",
            "22        0.848727       1.049873       0.010000       \n",
            "23        0.821584       1.044415       0.010000       \n",
            "24        0.795346       1.049958       0.010000       \n",
            "25        0.770733       1.006109       0.010000       \n",
            "26        0.745620       1.021476       0.010000       \n",
            "27        0.718542       1.010662       0.010000       \n",
            "28        0.692030       1.022178       0.010000       \n",
            "29        0.671750       0.997217       0.010000       \n",
            "30        0.645622       1.036665       0.010000       \n",
            "31        0.555548       0.982200       0.001000       \n",
            "32        0.542029       0.984116       0.001000       \n",
            "33        0.535892       0.986546       0.001000       \n",
            "34        0.530775       0.990145       0.001000       \n",
            "35        0.525942       0.993771       0.001000       \n",
            "36        0.522106       0.995155       0.001000       \n",
            "37        0.517763       0.999845       0.001000       \n",
            "38        0.514083       1.000869       0.001000       \n",
            "39        0.510210       1.001393       0.001000       \n",
            "40        0.506580       1.003838       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=SGD, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.125288       1.919380       0.010000       \n",
            "2         1.817419       1.730340       0.010000       \n",
            "3         1.662985       1.607577       0.010000       \n",
            "4         1.554604       1.520546       0.010000       \n",
            "5         1.461476       1.431585       0.010000       \n",
            "6         1.389934       1.385154       0.010000       \n",
            "7         1.334471       1.356437       0.010000       \n",
            "8         1.290939       1.306609       0.010000       \n",
            "9         1.250849       1.345243       0.010000       \n",
            "10        1.215178       1.237446       0.010000       \n",
            "11        1.179003       1.242227       0.010000       \n",
            "12        1.144559       1.207465       0.010000       \n",
            "13        1.112597       1.179204       0.010000       \n",
            "14        1.080527       1.191718       0.010000       \n",
            "15        1.048131       1.150295       0.010000       \n",
            "16        1.019749       1.146330       0.010000       \n",
            "17        0.987586       1.118247       0.010000       \n",
            "18        0.958655       1.079685       0.010000       \n",
            "19        0.928473       1.112105       0.010000       \n",
            "20        0.897919       1.059427       0.010000       \n",
            "21        0.873266       1.046707       0.010000       \n",
            "22        0.847820       1.040914       0.010000       \n",
            "23        0.818498       1.027126       0.010000       \n",
            "24        0.790679       1.014342       0.010000       \n",
            "25        0.766569       1.017335       0.010000       \n",
            "26        0.738623       1.016425       0.010000       \n",
            "27        0.713315       1.058240       0.010000       \n",
            "28        0.691374       1.010267       0.010000       \n",
            "29        0.662883       1.016425       0.010000       \n",
            "30        0.638555       1.009535       0.010000       \n",
            "31        0.547300       0.980138       0.001000       \n",
            "32        0.533024       0.985952       0.001000       \n",
            "33        0.527070       0.987625       0.001000       \n",
            "34        0.521640       0.991509       0.001000       \n",
            "35        0.517084       0.993768       0.001000       \n",
            "36        0.512748       0.996001       0.001000       \n",
            "37        0.508385       0.994594       0.001000       \n",
            "38        0.504433       1.001828       0.001000       \n",
            "39        0.500812       1.000144       0.001000       \n",
            "40        0.496584       1.005642       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=RMSProp, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         30.135576      1.810330       0.010000       \n",
            "2         1.953224       2.294995       0.010000       \n",
            "3         2.088476       1.835023       0.010000       \n",
            "4         2.140729       1.893461       0.010000       \n",
            "5         1.778017       1.784880       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=RMSProp, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         14.390164      2.103510       0.010000       \n",
            "2         1.954055       1.998827       0.010000       \n",
            "3         1.827405       1.708580       0.010000       \n",
            "4         1.788507       1.688954       0.010000       \n",
            "5         1.638129       1.633648       0.010000       \n",
            "6         1.592689       1.611543       0.010000       \n",
            "7         1.547572       1.583972       0.010000       \n",
            "8         1.499818       2.562989       0.010000       \n",
            "9         1.454985       1.497395       0.010000       \n",
            "10        1.385557       1.491720       0.010000       \n",
            "11        1.361677       1.775018       0.010000       \n",
            "12        1.329861       1.521874       0.010000       \n",
            "13        1.267198       1.477137       0.010000       \n",
            "14        1.276009       1.550859       0.010000       \n",
            "15        1.490768       1.521746       0.010000       \n",
            "16        1.142329       1.532592       0.010000       \n",
            "17        1.202520       1.525642       0.010000       \n",
            "18        1.106907       1.602062       0.010000       \n",
            "19        1.075583       1.633029       0.010000       \n",
            "20        1.038233       1.597466       0.010000       \n",
            "21        1.002248       1.656500       0.010000       \n",
            "22        0.978019       1.804513       0.010000       \n",
            "Early stopping at epoch 23\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=RMSProp, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         19.073660      1.740921       0.010000       \n",
            "2         1.784162       1.660006       0.010000       \n",
            "3         1.752435       1.683691       0.010000       \n",
            "4         1.587367       1.623815       0.010000       \n",
            "5         1.457709       1.497707       0.010000       \n",
            "6         1.403216       1.578773       0.010000       \n",
            "7         1.407857       1.411825       0.010000       \n",
            "8         1.271156       1.358248       0.010000       \n",
            "9         1.221309       1.459205       0.010000       \n",
            "10        1.241103       1.337422       0.010000       \n",
            "11        1.143953       1.416298       0.010000       \n",
            "12        1.127034       1.404619       0.010000       \n",
            "13        1.072157       1.381299       0.010000       \n",
            "14        1.099980       1.434603       0.010000       \n",
            "15        1.113380       1.505606       0.010000       \n",
            "16        1.035858       1.436645       0.010000       \n",
            "17        0.985806       1.514800       0.010000       \n",
            "18        0.960709       1.509378       0.010000       \n",
            "19        0.980162       1.429825       0.010000       \n",
            "Early stopping at epoch 20\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=RMSProp, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         22.876860      2.106540       0.010000       \n",
            "2         1.822880       1.711415       0.010000       \n",
            "3         1.769464       1.682027       0.010000       \n",
            "4         1.611706       1.554774       0.010000       \n",
            "5         1.490558       1.553202       0.010000       \n",
            "6         1.455622       1.577930       0.010000       \n",
            "7         2.021283       2.305486       0.010000       \n",
            "8         2.310014       2.303223       0.010000       \n",
            "9         2.303069       2.303412       0.010000       \n",
            "10        2.303708       2.302586       0.010000       \n",
            "11        2.303657       2.303235       0.010000       \n",
            "12        2.303484       2.303605       0.010000       \n",
            "13        2.303510       2.302757       0.010000       \n",
            "14        2.303541       2.303328       0.010000       \n",
            "Early stopping at epoch 15\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=RMSProp, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         18.313806      1.836573       0.010000       \n",
            "2         2.012753       2.248481       0.010000       \n",
            "3         1.916001       1.653379       0.010000       \n",
            "4         1.650537       1.611086       0.010000       \n",
            "5         1.538958       1.543094       0.010000       \n",
            "6         1.642416       1.506857       0.010000       \n",
            "7         1.429901       1.524421       0.010000       \n",
            "8         1.345158       1.404430       0.010000       \n",
            "9         1.247229       1.403705       0.010000       \n",
            "10        1.208583       1.432129       0.010000       \n",
            "11        1.141601       1.343919       0.010000       \n",
            "12        1.099121       1.372566       0.010000       \n",
            "13        1.058758       1.481769       0.010000       \n",
            "14        1.085877       1.500069       0.010000       \n",
            "15        1.005554       1.479517       0.010000       \n",
            "16        0.998503       1.438393       0.010000       \n",
            "17        1.170924       1.497759       0.010000       \n",
            "18        0.938647       1.543237       0.010000       \n",
            "19        0.904228       1.643426       0.010000       \n",
            "20        0.955918       1.517492       0.010000       \n",
            "Early stopping at epoch 21\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=Adam, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.855751       1.747668       0.010000       \n",
            "2         1.639729       1.611685       0.010000       \n",
            "3         1.546059       1.624747       0.010000       \n",
            "4         1.500832       1.520303       0.010000       \n",
            "5         1.442741       1.477300       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=Adam, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.884619       1.707510       0.010000       \n",
            "2         1.647183       1.626303       0.010000       \n",
            "3         1.579526       1.570835       0.010000       \n",
            "4         1.544265       1.589629       0.010000       \n",
            "5         1.512236       1.546358       0.010000       \n",
            "6         1.477260       1.534494       0.010000       \n",
            "7         1.440094       1.498565       0.010000       \n",
            "8         1.438419       1.494129       0.010000       \n",
            "9         1.402464       1.509158       0.010000       \n",
            "10        1.384393       1.510977       0.010000       \n",
            "11        1.367887       1.477528       0.010000       \n",
            "12        1.346468       1.480496       0.010000       \n",
            "13        1.327788       1.437379       0.010000       \n",
            "14        1.313535       1.439957       0.010000       \n",
            "15        1.286805       1.469316       0.010000       \n",
            "16        1.263965       1.477606       0.010000       \n",
            "17        1.253720       1.472239       0.010000       \n",
            "18        1.229491       1.456946       0.010000       \n",
            "19        1.219556       1.498887       0.010000       \n",
            "20        1.190564       1.493237       0.010000       \n",
            "21        1.172006       1.488534       0.010000       \n",
            "22        1.137604       1.459043       0.010000       \n",
            "Early stopping at epoch 23\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=Adam, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.899775       1.675221       0.010000       \n",
            "2         1.625882       1.785335       0.010000       \n",
            "3         1.543527       1.552144       0.010000       \n",
            "4         1.487537       1.529981       0.010000       \n",
            "5         1.452050       1.542420       0.010000       \n",
            "6         1.424960       1.522524       0.010000       \n",
            "7         1.397700       1.501068       0.010000       \n",
            "8         1.366567       1.474731       0.010000       \n",
            "9         1.358293       1.515324       0.010000       \n",
            "10        1.331211       1.502107       0.010000       \n",
            "11        1.326711       1.475732       0.010000       \n",
            "12        1.292627       1.458437       0.010000       \n",
            "13        1.326325       1.784408       0.010000       \n",
            "14        1.307346       1.543234       0.010000       \n",
            "15        1.242500       1.449661       0.010000       \n",
            "16        1.224352       1.542018       0.010000       \n",
            "17        1.219732       1.563955       0.010000       \n",
            "18        1.196695       1.536971       0.010000       \n",
            "19        1.197062       1.534402       0.010000       \n",
            "20        1.178329       1.543654       0.010000       \n",
            "21        1.160116       1.567621       0.010000       \n",
            "22        1.145001       1.543382       0.010000       \n",
            "23        1.132916       1.551681       0.010000       \n",
            "24        1.125736       1.633803       0.010000       \n",
            "Early stopping at epoch 25\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=Adam, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.826146       1.695271       0.010000       \n",
            "2         1.563046       1.634407       0.010000       \n",
            "3         1.473630       1.477364       0.010000       \n",
            "4         1.595356       1.631437       0.010000       \n",
            "5         1.439814       1.466059       0.010000       \n",
            "6         1.353913       1.482180       0.010000       \n",
            "7         1.318978       1.461828       0.010000       \n",
            "8         1.294719       1.445353       0.010000       \n",
            "9         1.254503       1.422935       0.010000       \n",
            "10        1.254045       1.470755       0.010000       \n",
            "11        1.227819       1.432845       0.010000       \n",
            "12        1.196300       1.506324       0.010000       \n",
            "13        1.180844       1.426257       0.010000       \n",
            "14        1.174515       1.425255       0.010000       \n",
            "15        1.158993       1.491681       0.010000       \n",
            "16        1.134402       1.482874       0.010000       \n",
            "17        1.146190       1.450734       0.010000       \n",
            "18        1.113509       1.540464       0.010000       \n",
            "Early stopping at epoch 19\n",
            "\n",
            "Running experiment: Kernel Size=5, Pooling=avg, Optimizer=Adam, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.322509       2.303576       0.010000       \n",
            "2         2.303609       2.302920       0.010000       \n",
            "3         2.303648       2.303853       0.010000       \n",
            "4         2.303631       2.303230       0.010000       \n",
            "5         2.303598       2.304156       0.010000       \n",
            "6         2.303696       2.304019       0.010000       \n",
            "7         2.303588       2.304066       0.010000       \n",
            "8         2.303718       2.304072       0.010000       \n",
            "9         2.303537       2.303048       0.010000       \n",
            "10        2.303656       2.304021       0.010000       \n",
            "11        2.303743       2.303821       0.010000       \n",
            "Early stopping at epoch 12\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=SGD, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.035932       1.803817       0.010000       \n",
            "2         1.654129       1.541153       0.010000       \n",
            "3         1.469783       1.475675       0.010000       \n",
            "4         1.368110       1.342422       0.010000       \n",
            "5         1.280079       1.291115       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=SGD, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.029621       1.787314       0.010000       \n",
            "2         1.624958       1.535236       0.010000       \n",
            "3         1.471153       1.423653       0.010000       \n",
            "4         1.377883       1.369359       0.010000       \n",
            "5         1.289988       1.304073       0.010000       \n",
            "6         1.214457       1.230107       0.010000       \n",
            "7         1.142011       1.155651       0.010000       \n",
            "8         1.079228       1.122549       0.010000       \n",
            "9         1.019848       1.055520       0.010000       \n",
            "10        0.967131       1.045409       0.010000       \n",
            "11        0.921929       1.002848       0.010000       \n",
            "12        0.879832       1.002107       0.010000       \n",
            "13        0.841212       0.949726       0.010000       \n",
            "14        0.801149       0.937183       0.010000       \n",
            "15        0.765662       0.926218       0.010000       \n",
            "16        0.732315       0.932539       0.010000       \n",
            "17        0.692887       0.952720       0.010000       \n",
            "18        0.661673       0.903262       0.010000       \n",
            "19        0.628352       0.885406       0.010000       \n",
            "20        0.591092       0.952923       0.010000       \n",
            "21        0.560103       0.921896       0.010000       \n",
            "22        0.525086       0.911499       0.010000       \n",
            "23        0.493295       0.908500       0.010000       \n",
            "24        0.457380       0.918004       0.010000       \n",
            "25        0.424408       0.993301       0.010000       \n",
            "26        0.394135       1.001682       0.010000       \n",
            "27        0.363000       0.986123       0.010000       \n",
            "28        0.329915       0.997036       0.010000       \n",
            "Early stopping at epoch 29\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=SGD, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.054464       1.816481       0.010000       \n",
            "2         1.662967       1.572079       0.010000       \n",
            "3         1.490849       1.447871       0.010000       \n",
            "4         1.389723       1.371787       0.010000       \n",
            "5         1.306698       1.304146       0.010000       \n",
            "6         1.229073       1.254655       0.010000       \n",
            "7         1.162711       1.190937       0.010000       \n",
            "8         1.105540       1.154651       0.010000       \n",
            "9         1.048252       1.091503       0.010000       \n",
            "10        0.997999       1.067751       0.010000       \n",
            "11        0.950163       1.032100       0.010000       \n",
            "12        0.910942       1.017510       0.010000       \n",
            "13        0.866736       0.986234       0.010000       \n",
            "14        0.827044       0.984931       0.010000       \n",
            "15        0.789661       0.946785       0.010000       \n",
            "16        0.751745       0.945015       0.010000       \n",
            "17        0.714989       0.932535       0.010000       \n",
            "18        0.676917       0.905066       0.010000       \n",
            "19        0.643774       0.902006       0.010000       \n",
            "20        0.608492       0.965017       0.010000       \n",
            "21        0.573411       0.920854       0.010000       \n",
            "22        0.538002       1.007461       0.010000       \n",
            "23        0.503657       0.946441       0.010000       \n",
            "24        0.468167       0.950500       0.010000       \n",
            "25        0.432149       0.964311       0.010000       \n",
            "26        0.402540       0.976551       0.010000       \n",
            "27        0.365136       0.987508       0.010000       \n",
            "28        0.336077       1.052031       0.010000       \n",
            "Early stopping at epoch 29\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=SGD, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.047302       1.807858       0.010000       \n",
            "2         1.666903       1.570142       0.010000       \n",
            "3         1.485093       1.438753       0.010000       \n",
            "4         1.379364       1.352235       0.010000       \n",
            "5         1.292020       1.293005       0.010000       \n",
            "6         1.223493       1.228910       0.010000       \n",
            "7         1.156351       1.188547       0.010000       \n",
            "8         1.102592       1.116992       0.010000       \n",
            "9         1.047810       1.086000       0.010000       \n",
            "10        0.997222       1.116091       0.010000       \n",
            "11        0.949251       1.017110       0.010000       \n",
            "12        0.903961       1.006680       0.010000       \n",
            "13        0.861573       0.990880       0.010000       \n",
            "14        0.823298       0.961364       0.010000       \n",
            "15        0.781622       0.944581       0.010000       \n",
            "16        0.748032       0.939232       0.010000       \n",
            "17        0.706308       0.909378       0.010000       \n",
            "18        0.670205       0.936701       0.010000       \n",
            "19        0.634523       0.915406       0.010000       \n",
            "20        0.599546       0.933779       0.010000       \n",
            "21        0.563266       0.931145       0.010000       \n",
            "22        0.527775       0.921645       0.010000       \n",
            "23        0.496223       0.927511       0.010000       \n",
            "24        0.461302       0.973675       0.010000       \n",
            "25        0.429896       1.031307       0.010000       \n",
            "26        0.395804       0.948682       0.010000       \n",
            "Early stopping at epoch 27\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=SGD, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.019968       1.778943       0.010000       \n",
            "2         1.646820       1.546431       0.010000       \n",
            "3         1.476047       1.455042       0.010000       \n",
            "4         1.376542       1.370607       0.010000       \n",
            "5         1.297440       1.289011       0.010000       \n",
            "6         1.224600       1.269494       0.010000       \n",
            "7         1.158104       1.200092       0.010000       \n",
            "8         1.096243       1.125340       0.010000       \n",
            "9         1.043607       1.096293       0.010000       \n",
            "10        0.992472       1.130685       0.010000       \n",
            "11        0.948065       1.016418       0.010000       \n",
            "12        0.903857       0.999033       0.010000       \n",
            "13        0.863677       0.992890       0.010000       \n",
            "14        0.825014       0.961851       0.010000       \n",
            "15        0.790255       0.942769       0.010000       \n",
            "16        0.753542       0.989396       0.010000       \n",
            "17        0.718448       0.919071       0.010000       \n",
            "18        0.682580       0.950201       0.010000       \n",
            "19        0.647193       0.929315       0.010000       \n",
            "20        0.610043       0.942258       0.010000       \n",
            "21        0.579275       0.911073       0.010000       \n",
            "22        0.543416       0.900110       0.010000       \n",
            "23        0.508727       0.929759       0.010000       \n",
            "24        0.476935       0.933657       0.010000       \n",
            "25        0.440271       0.969307       0.010000       \n",
            "26        0.410322       0.952639       0.010000       \n",
            "27        0.377220       0.987430       0.010000       \n",
            "28        0.345251       1.031665       0.010000       \n",
            "29        0.311406       1.031980       0.010000       \n",
            "30        0.283394       1.088417       0.010000       \n",
            "31        0.182868       1.024854       0.001000       \n",
            "Early stopping at epoch 32\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=RMSProp, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         38.048676      2.303971       0.010000       \n",
            "2         2.303743       2.304239       0.010000       \n",
            "3         2.303722       2.303354       0.010000       \n",
            "4         2.303610       2.303371       0.010000       \n",
            "5         2.303527       2.304669       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=RMSProp, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         76.901115      2.303514       0.010000       \n",
            "2         2.332625       2.273904       0.010000       \n",
            "3         3.207434       2.219355       0.010000       \n",
            "4         2.141810       2.052446       0.010000       \n",
            "5         2.048018       1.999746       0.010000       \n",
            "6         2.020541       1.999808       0.010000       \n",
            "7         2.007648       2.025293       0.010000       \n",
            "8         2.002757       1.953384       0.010000       \n",
            "9         2.051303       1.959591       0.010000       \n",
            "10        1.974868       1.986475       0.010000       \n",
            "11        1.962379       1.952437       0.010000       \n",
            "12        1.966906       2.030118       0.010000       \n",
            "13        1.952547       1.933439       0.010000       \n",
            "14        2.173954       1.938766       0.010000       \n",
            "15        1.953897       1.924645       0.010000       \n",
            "16        1.950781       1.959309       0.010000       \n",
            "17        1.961174       1.927662       0.010000       \n",
            "18        1.947426       1.927370       0.010000       \n",
            "19        1.929983       1.955956       0.010000       \n",
            "20        1.956285       1.936166       0.010000       \n",
            "21        1.947574       1.907482       0.010000       \n",
            "22        1.962427       1.960514       0.010000       \n",
            "23        1.934459       1.996900       0.010000       \n",
            "24        1.938454       1.977345       0.010000       \n",
            "25        1.941354       1.951821       0.010000       \n",
            "26        1.948814       1.941544       0.010000       \n",
            "27        1.935970       1.904559       0.010000       \n",
            "28        1.930852       1.919556       0.010000       \n",
            "29        1.929302       1.926712       0.010000       \n",
            "30        1.929321       1.996637       0.010000       \n",
            "31        1.851542       1.858565       0.001000       \n",
            "32        1.823506       1.845627       0.001000       \n",
            "33        1.808188       1.830535       0.001000       \n",
            "34        1.799759       1.825158       0.001000       \n",
            "35        1.790744       1.814124       0.001000       \n",
            "36        1.783296       1.813380       0.001000       \n",
            "37        1.775741       1.813633       0.001000       \n",
            "38        1.770833       1.812468       0.001000       \n",
            "39        1.765634       1.797527       0.001000       \n",
            "40        1.754831       1.799224       0.001000       \n",
            "41        1.744973       1.778191       0.001000       \n",
            "42        1.726083       1.759591       0.001000       \n",
            "43        1.714722       1.756957       0.001000       \n",
            "44        1.706009       1.753581       0.001000       \n",
            "45        1.699838       1.735931       0.001000       \n",
            "46        1.689879       1.734112       0.001000       \n",
            "47        1.682169       1.724275       0.001000       \n",
            "48        1.671630       1.720403       0.001000       \n",
            "49        1.664602       1.717505       0.001000       \n",
            "50        1.655566       1.713020       0.001000       \n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=RMSProp, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         120.277318     2.109434       0.010000       \n",
            "2         2.283458       2.238783       0.010000       \n",
            "3         2.189171       2.121153       0.010000       \n",
            "4         2.099485       2.136997       0.010000       \n",
            "5         2.075889       2.069472       0.010000       \n",
            "6         2.062075       2.003246       0.010000       \n",
            "7         2.007027       1.990767       0.010000       \n",
            "8         1.972733       1.992857       0.010000       \n",
            "9         2.032568       1.985702       0.010000       \n",
            "10        1.951798       1.929730       0.010000       \n",
            "11        1.923919       1.895556       0.010000       \n",
            "12        1.910132       1.874784       0.010000       \n",
            "13        1.881043       1.845768       0.010000       \n",
            "14        1.853389       1.957655       0.010000       \n",
            "15        1.859246       1.823213       0.010000       \n",
            "16        1.795700       1.804136       0.010000       \n",
            "17        1.814815       1.801617       0.010000       \n",
            "18        1.810493       1.792079       0.010000       \n",
            "19        1.798685       1.773983       0.010000       \n",
            "20        1.793368       1.836776       0.010000       \n",
            "21        1.764293       1.785472       0.010000       \n",
            "22        1.782469       1.758987       0.010000       \n",
            "23        1.769996       1.796507       0.010000       \n",
            "24        1.738256       1.914428       0.010000       \n",
            "25        1.746521       1.790203       0.010000       \n",
            "26        1.804328       1.766897       0.010000       \n",
            "27        1.832266       1.780131       0.010000       \n",
            "28        1.741283       1.747188       0.010000       \n",
            "29        1.740291       1.805723       0.010000       \n",
            "30        1.759951       1.804447       0.010000       \n",
            "31        1.648694       1.680520       0.001000       \n",
            "32        1.602815       1.664215       0.001000       \n",
            "33        1.586452       1.662839       0.001000       \n",
            "34        1.574980       1.654711       0.001000       \n",
            "35        1.568268       1.650387       0.001000       \n",
            "36        1.560620       1.636841       0.001000       \n",
            "37        1.554204       1.639804       0.001000       \n",
            "38        1.548195       1.636034       0.001000       \n",
            "39        1.541838       1.626752       0.001000       \n",
            "40        1.539334       1.629457       0.001000       \n",
            "41        1.535044       1.636488       0.001000       \n",
            "42        1.532439       1.625281       0.001000       \n",
            "43        1.526683       1.644009       0.001000       \n",
            "44        1.523749       1.626996       0.001000       \n",
            "45        1.519691       1.632017       0.001000       \n",
            "46        1.517026       1.623368       0.001000       \n",
            "47        1.513413       1.626101       0.001000       \n",
            "48        1.509355       1.627136       0.001000       \n",
            "49        1.507888       1.624163       0.001000       \n",
            "50        1.504048       1.627025       0.001000       \n",
            "51        1.501962       1.628126       0.001000       \n",
            "52        1.500139       1.638693       0.001000       \n",
            "53        1.495735       1.618744       0.001000       \n",
            "54        1.491462       1.615000       0.001000       \n",
            "55        1.491247       1.618033       0.001000       \n",
            "56        1.486766       1.622661       0.001000       \n",
            "57        1.483607       1.627850       0.001000       \n",
            "58        1.482202       1.616708       0.001000       \n",
            "59        1.478912       1.597363       0.001000       \n",
            "60        1.475034       1.616692       0.001000       \n",
            "61        1.457302       1.615665       0.000100       \n",
            "62        1.454407       1.614793       0.000100       \n",
            "63        1.452937       1.612883       0.000100       \n",
            "64        1.452113       1.610502       0.000100       \n",
            "65        1.452045       1.612754       0.000100       \n",
            "66        1.450820       1.614381       0.000100       \n",
            "67        1.450606       1.612089       0.000100       \n",
            "68        1.449514       1.614409       0.000100       \n",
            "Early stopping at epoch 69\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=RMSProp, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         90.465102      2.268887       0.010000       \n",
            "2         2.252737       2.208847       0.010000       \n",
            "3         2.969752       2.148288       0.010000       \n",
            "4         2.162498       2.162809       0.010000       \n",
            "5         2.125317       2.090176       0.010000       \n",
            "6         2.062554       2.039483       0.010000       \n",
            "7         2.053327       1.968412       0.010000       \n",
            "8         1.972255       1.979704       0.010000       \n",
            "9         1.967997       1.917977       0.010000       \n",
            "10        1.923442       1.903108       0.010000       \n",
            "11        1.940338       1.906123       0.010000       \n",
            "12        1.925959       1.898064       0.010000       \n",
            "13        1.912245       1.928740       0.010000       \n",
            "14        1.905988       1.861881       0.010000       \n",
            "15        1.912986       1.863557       0.010000       \n",
            "16        1.878561       1.844947       0.010000       \n",
            "17        1.898824       1.884960       0.010000       \n",
            "18        1.880914       1.871152       0.010000       \n",
            "19        1.872984       1.858648       0.010000       \n",
            "20        1.875424       1.900389       0.010000       \n",
            "21        1.842342       1.876006       0.010000       \n",
            "22        1.913427       1.840522       0.010000       \n",
            "23        1.859397       1.942632       0.010000       \n",
            "24        1.826071       1.799148       0.010000       \n",
            "25        1.809114       1.836900       0.010000       \n",
            "26        1.799073       1.902221       0.010000       \n",
            "27        1.825417       1.773452       0.010000       \n",
            "28        1.780127       1.748686       0.010000       \n",
            "29        1.791098       1.755418       0.010000       \n",
            "30        1.753098       1.785629       0.010000       \n",
            "31        1.630089       1.672099       0.001000       \n",
            "32        1.589294       1.648465       0.001000       \n",
            "33        1.569750       1.632284       0.001000       \n",
            "34        1.555193       1.631487       0.001000       \n",
            "35        1.543662       1.626064       0.001000       \n",
            "36        1.532822       1.620453       0.001000       \n",
            "37        1.521922       1.604080       0.001000       \n",
            "38        1.509916       1.602479       0.001000       \n",
            "39        1.497964       1.598049       0.001000       \n",
            "40        1.488708       1.583020       0.001000       \n",
            "41        1.477141       1.580071       0.001000       \n",
            "42        1.466576       1.565034       0.001000       \n",
            "43        1.456710       1.561194       0.001000       \n",
            "44        1.442714       1.565374       0.001000       \n",
            "45        1.429681       1.550509       0.001000       \n",
            "46        1.419775       1.550053       0.001000       \n",
            "47        1.407954       1.551697       0.001000       \n",
            "48        1.398781       1.526390       0.001000       \n",
            "49        1.386017       1.530018       0.001000       \n",
            "50        1.377015       1.522118       0.001000       \n",
            "51        1.366472       1.517241       0.001000       \n",
            "52        1.357585       1.504781       0.001000       \n",
            "53        1.345981       1.506839       0.001000       \n",
            "54        1.336069       1.488291       0.001000       \n",
            "55        1.326578       1.486414       0.001000       \n",
            "56        1.318890       1.472174       0.001000       \n",
            "57        1.310609       1.477646       0.001000       \n",
            "58        1.300797       1.476750       0.001000       \n",
            "59        1.291429       1.465595       0.001000       \n",
            "60        1.283248       1.460342       0.001000       \n",
            "61        1.251429       1.458036       0.000100       \n",
            "62        1.243695       1.459380       0.000100       \n",
            "63        1.240833       1.459724       0.000100       \n",
            "64        1.238229       1.459780       0.000100       \n",
            "65        1.236539       1.467599       0.000100       \n",
            "66        1.234870       1.464570       0.000100       \n",
            "67        1.233473       1.471009       0.000100       \n",
            "68        1.232251       1.462992       0.000100       \n",
            "69        1.230778       1.462819       0.000100       \n",
            "70        1.229677       1.470709       0.000100       \n",
            "Early stopping at epoch 71\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=RMSProp, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         36.877452      2.304873       0.010000       \n",
            "2         2.303870       2.303322       0.010000       \n",
            "3         2.303658       2.303365       0.010000       \n",
            "4         2.303738       2.303539       0.010000       \n",
            "5         2.303581       2.303285       0.010000       \n",
            "6         2.303705       2.303824       0.010000       \n",
            "7         2.303804       2.303037       0.010000       \n",
            "8         2.303744       2.304087       0.010000       \n",
            "9         2.303567       2.305009       0.010000       \n",
            "10        2.303629       2.303337       0.010000       \n",
            "11        2.303508       2.303204       0.010000       \n",
            "12        2.303551       2.303187       0.010000       \n",
            "13        2.303548       2.302889       0.010000       \n",
            "14        2.303530       2.303689       0.010000       \n",
            "15        2.303578       2.302928       0.010000       \n",
            "16        2.303655       2.303638       0.010000       \n",
            "17        2.303483       2.302865       0.010000       \n",
            "18        2.303783       2.303139       0.010000       \n",
            "19        2.303795       2.303204       0.010000       \n",
            "20        2.303661       2.303192       0.010000       \n",
            "21        2.303427       2.304500       0.010000       \n",
            "22        2.303372       2.304663       0.010000       \n",
            "23        2.303808       2.304263       0.010000       \n",
            "24        2.303759       2.303543       0.010000       \n",
            "25        2.303617       2.303624       0.010000       \n",
            "26        2.303703       2.302879       0.010000       \n",
            "Early stopping at epoch 27\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=Adam, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.041826       1.872276       0.010000       \n",
            "2         1.853589       1.796085       0.010000       \n",
            "3         1.822246       1.815363       0.010000       \n",
            "4         1.804002       1.801789       0.010000       \n",
            "5         1.784135       1.741153       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=Adam, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.380065       2.303576       0.010000       \n",
            "2         2.303621       2.304379       0.010000       \n",
            "3         2.303736       2.303842       0.010000       \n",
            "4         2.303544       2.302738       0.010000       \n",
            "5         2.303546       2.303293       0.010000       \n",
            "6         2.303252       2.303535       0.010000       \n",
            "7         2.303560       2.302855       0.010000       \n",
            "8         2.303714       2.302987       0.010000       \n",
            "9         2.303605       2.303240       0.010000       \n",
            "10        2.303648       2.303869       0.010000       \n",
            "11        2.303649       2.303258       0.010000       \n",
            "12        2.303569       2.304077       0.010000       \n",
            "13        2.303517       2.302911       0.010000       \n",
            "Early stopping at epoch 14\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=Adam, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.118306       2.016610       0.010000       \n",
            "2         1.901548       1.893342       0.010000       \n",
            "3         1.834713       1.797104       0.010000       \n",
            "4         1.792064       1.789985       0.010000       \n",
            "5         1.771658       1.784015       0.010000       \n",
            "6         1.751151       1.759240       0.010000       \n",
            "7         1.737254       1.739774       0.010000       \n",
            "8         1.714186       1.727067       0.010000       \n",
            "9         1.703851       1.729684       0.010000       \n",
            "10        1.699119       1.716499       0.010000       \n",
            "11        1.745285       1.734857       0.010000       \n",
            "12        1.678563       1.672604       0.010000       \n",
            "13        1.664547       1.673165       0.010000       \n",
            "14        1.658455       1.711653       0.010000       \n",
            "15        1.655719       1.654609       0.010000       \n",
            "16        1.643631       1.666307       0.010000       \n",
            "17        1.651776       1.706342       0.010000       \n",
            "18        1.630010       1.651601       0.010000       \n",
            "19        1.624441       1.668365       0.010000       \n",
            "20        1.611225       1.617226       0.010000       \n",
            "21        1.611509       1.674532       0.010000       \n",
            "22        1.596529       1.627968       0.010000       \n",
            "23        1.588331       1.685667       0.010000       \n",
            "24        1.584688       1.640549       0.010000       \n",
            "25        1.580411       1.616795       0.010000       \n",
            "26        1.570272       1.594764       0.010000       \n",
            "27        1.562728       1.585437       0.010000       \n",
            "28        1.545905       1.627865       0.010000       \n",
            "29        1.580086       1.607822       0.010000       \n",
            "30        1.574325       1.598833       0.010000       \n",
            "31        1.433524       1.476043       0.001000       \n",
            "32        1.379933       1.453758       0.001000       \n",
            "33        1.358595       1.448076       0.001000       \n",
            "34        1.345588       1.436998       0.001000       \n",
            "35        1.333475       1.439903       0.001000       \n",
            "36        1.325115       1.429170       0.001000       \n",
            "37        1.316465       1.422195       0.001000       \n",
            "38        1.309909       1.427413       0.001000       \n",
            "39        1.304115       1.420666       0.001000       \n",
            "40        1.300762       1.419833       0.001000       \n",
            "41        1.291852       1.427288       0.001000       \n",
            "42        1.291006       1.417514       0.001000       \n",
            "43        1.279449       1.423967       0.001000       \n",
            "44        1.273293       1.415189       0.001000       \n",
            "45        1.266544       1.418478       0.001000       \n",
            "46        1.257942       1.411732       0.001000       \n",
            "47        1.248418       1.406850       0.001000       \n",
            "48        1.240817       1.402892       0.001000       \n",
            "49        1.232965       1.407117       0.001000       \n",
            "50        1.227643       1.414160       0.001000       \n",
            "51        1.218935       1.397114       0.001000       \n",
            "52        1.213357       1.407359       0.001000       \n",
            "53        1.207525       1.405597       0.001000       \n",
            "54        1.200706       1.405755       0.001000       \n",
            "55        1.201076       1.403594       0.001000       \n",
            "56        1.191922       1.413319       0.001000       \n",
            "57        1.183455       1.410371       0.001000       \n",
            "58        1.178228       1.407508       0.001000       \n",
            "59        1.174022       1.417733       0.001000       \n",
            "60        1.169006       1.417909       0.001000       \n",
            "Early stopping at epoch 61\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=Adam, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.394690       2.303598       0.010000       \n",
            "2         2.303406       2.304025       0.010000       \n",
            "3         2.303627       2.304137       0.010000       \n",
            "4         2.303489       2.303321       0.010000       \n",
            "5         2.303548       2.303617       0.010000       \n",
            "6         2.303590       2.303842       0.010000       \n",
            "7         2.303279       2.302787       0.010000       \n",
            "8         2.303644       2.303309       0.010000       \n",
            "9         2.303487       2.302823       0.010000       \n",
            "10        2.303620       2.304364       0.010000       \n",
            "11        2.303709       2.303879       0.010000       \n",
            "12        2.303687       2.304520       0.010000       \n",
            "13        2.303395       2.302974       0.010000       \n",
            "14        2.303457       2.303196       0.010000       \n",
            "15        2.303525       2.302789       0.010000       \n",
            "16        2.303441       2.303171       0.010000       \n",
            "Early stopping at epoch 17\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=max, Optimizer=Adam, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.354092       2.303410       0.010000       \n",
            "2         2.303628       2.303808       0.010000       \n",
            "3         2.303638       2.303703       0.010000       \n",
            "4         2.303529       2.303246       0.010000       \n",
            "5         2.303390       2.303545       0.010000       \n",
            "6         2.303704       2.303496       0.010000       \n",
            "7         2.303571       2.303844       0.010000       \n",
            "8         2.303719       2.303021       0.010000       \n",
            "9         2.303560       2.304112       0.010000       \n",
            "10        2.303641       2.303952       0.010000       \n",
            "11        2.303562       2.303364       0.010000       \n",
            "12        2.303750       2.303087       0.010000       \n",
            "13        2.303405       2.303638       0.010000       \n",
            "14        2.303379       2.302784       0.010000       \n",
            "15        2.303478       2.303399       0.010000       \n",
            "16        2.303641       2.304524       0.010000       \n",
            "17        2.303554       2.304123       0.010000       \n",
            "18        2.303639       2.305445       0.010000       \n",
            "19        2.303707       2.303150       0.010000       \n",
            "20        2.303867       2.303758       0.010000       \n",
            "21        2.303545       2.302907       0.010000       \n",
            "22        2.303722       2.303897       0.010000       \n",
            "23        2.303611       2.303116       0.010000       \n",
            "Early stopping at epoch 24\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=SGD, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.109960       1.894889       0.010000       \n",
            "2         1.779591       1.681614       0.010000       \n",
            "3         1.602338       1.575338       0.010000       \n",
            "4         1.483924       1.466497       0.010000       \n",
            "5         1.407555       1.415506       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=SGD, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.098237       1.885645       0.010000       \n",
            "2         1.767113       1.697721       0.010000       \n",
            "3         1.599830       1.532102       0.010000       \n",
            "4         1.480096       1.443451       0.010000       \n",
            "5         1.403280       1.382893       0.010000       \n",
            "6         1.347514       1.358513       0.010000       \n",
            "7         1.298497       1.309969       0.010000       \n",
            "8         1.253747       1.283390       0.010000       \n",
            "9         1.212404       1.241713       0.010000       \n",
            "10        1.170105       1.214020       0.010000       \n",
            "11        1.131978       1.200733       0.010000       \n",
            "12        1.093268       1.161679       0.010000       \n",
            "13        1.057195       1.133285       0.010000       \n",
            "14        1.021282       1.108485       0.010000       \n",
            "15        0.986685       1.083903       0.010000       \n",
            "16        0.951263       1.093768       0.010000       \n",
            "17        0.919365       1.065774       0.010000       \n",
            "18        0.890901       1.038377       0.010000       \n",
            "19        0.856751       1.049828       0.010000       \n",
            "20        0.829213       1.019295       0.010000       \n",
            "21        0.798020       1.053094       0.010000       \n",
            "22        0.770823       1.015034       0.010000       \n",
            "23        0.741821       1.001506       0.010000       \n",
            "24        0.710750       1.004998       0.010000       \n",
            "25        0.683495       0.994033       0.010000       \n",
            "26        0.655988       1.009999       0.010000       \n",
            "27        0.628637       1.026160       0.010000       \n",
            "28        0.599719       1.036180       0.010000       \n",
            "29        0.571539       1.041391       0.010000       \n",
            "30        0.543410       1.027112       0.010000       \n",
            "31        0.435482       1.004229       0.001000       \n",
            "32        0.419006       1.009836       0.001000       \n",
            "33        0.411897       1.016210       0.001000       \n",
            "34        0.406188       1.020025       0.001000       \n",
            "Early stopping at epoch 35\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=SGD, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.105964       1.877295       0.010000       \n",
            "2         1.754179       1.662499       0.010000       \n",
            "3         1.598440       1.565367       0.010000       \n",
            "4         1.495462       1.467664       0.010000       \n",
            "5         1.416032       1.456120       0.010000       \n",
            "6         1.359358       1.368903       0.010000       \n",
            "7         1.309716       1.342753       0.010000       \n",
            "8         1.261108       1.277083       0.010000       \n",
            "9         1.217172       1.268071       0.010000       \n",
            "10        1.171138       1.213058       0.010000       \n",
            "11        1.127999       1.180878       0.010000       \n",
            "12        1.088222       1.142437       0.010000       \n",
            "13        1.048993       1.130802       0.010000       \n",
            "14        1.011667       1.115652       0.010000       \n",
            "15        0.977553       1.073068       0.010000       \n",
            "16        0.943141       1.048280       0.010000       \n",
            "17        0.910992       1.037776       0.010000       \n",
            "18        0.880015       1.030832       0.010000       \n",
            "19        0.849281       1.004995       0.010000       \n",
            "20        0.819483       1.017559       0.010000       \n",
            "21        0.788528       1.009977       0.010000       \n",
            "22        0.759321       0.991563       0.010000       \n",
            "23        0.730363       0.967082       0.010000       \n",
            "24        0.702561       0.973036       0.010000       \n",
            "25        0.672710       1.002472       0.010000       \n",
            "26        0.646656       0.985354       0.010000       \n",
            "27        0.620510       0.981708       0.010000       \n",
            "28        0.589484       0.999603       0.010000       \n",
            "29        0.563326       0.984647       0.010000       \n",
            "30        0.536674       1.000368       0.010000       \n",
            "31        0.435396       0.953988       0.001000       \n",
            "32        0.420382       0.962785       0.001000       \n",
            "33        0.413411       0.970391       0.001000       \n",
            "34        0.408242       0.974765       0.001000       \n",
            "35        0.403093       0.976520       0.001000       \n",
            "36        0.398943       0.979838       0.001000       \n",
            "37        0.394307       0.983971       0.001000       \n",
            "38        0.390641       0.986385       0.001000       \n",
            "39        0.386237       0.990749       0.001000       \n",
            "40        0.381921       0.993030       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=SGD, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.098673       1.886020       0.010000       \n",
            "2         1.774237       1.695231       0.010000       \n",
            "3         1.608830       1.571106       0.010000       \n",
            "4         1.493119       1.464521       0.010000       \n",
            "5         1.415619       1.395364       0.010000       \n",
            "6         1.358456       1.358882       0.010000       \n",
            "7         1.307288       1.318223       0.010000       \n",
            "8         1.263394       1.291093       0.010000       \n",
            "9         1.223052       1.245962       0.010000       \n",
            "10        1.181146       1.230450       0.010000       \n",
            "11        1.142038       1.198261       0.010000       \n",
            "12        1.107803       1.182366       0.010000       \n",
            "13        1.070795       1.150708       0.010000       \n",
            "14        1.035966       1.121420       0.010000       \n",
            "15        1.001666       1.122714       0.010000       \n",
            "16        0.968986       1.096022       0.010000       \n",
            "17        0.939413       1.083920       0.010000       \n",
            "18        0.906961       1.069810       0.010000       \n",
            "19        0.877811       1.048002       0.010000       \n",
            "20        0.844241       1.049430       0.010000       \n",
            "21        0.817015       1.050141       0.010000       \n",
            "22        0.787882       1.027700       0.010000       \n",
            "23        0.759169       1.063092       0.010000       \n",
            "24        0.730437       1.021617       0.010000       \n",
            "25        0.701993       1.040591       0.010000       \n",
            "26        0.673486       1.029474       0.010000       \n",
            "27        0.643855       1.047090       0.010000       \n",
            "28        0.616076       1.036090       0.010000       \n",
            "29        0.589704       1.050563       0.010000       \n",
            "30        0.559627       1.102572       0.010000       \n",
            "31        0.454784       1.012356       0.001000       \n",
            "32        0.437972       1.024232       0.001000       \n",
            "33        0.430586       1.028504       0.001000       \n",
            "34        0.424849       1.030783       0.001000       \n",
            "35        0.419667       1.034489       0.001000       \n",
            "36        0.414961       1.037054       0.001000       \n",
            "37        0.410323       1.043990       0.001000       \n",
            "38        0.405331       1.049905       0.001000       \n",
            "39        0.401194       1.049184       0.001000       \n",
            "40        0.397008       1.052359       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=SGD, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.113286       1.899811       0.010000       \n",
            "2         1.782173       1.703817       0.010000       \n",
            "3         1.619045       1.583779       0.010000       \n",
            "4         1.512315       1.489735       0.010000       \n",
            "5         1.435975       1.425751       0.010000       \n",
            "6         1.377464       1.383914       0.010000       \n",
            "7         1.329970       1.339795       0.010000       \n",
            "8         1.280350       1.311565       0.010000       \n",
            "9         1.237668       1.265418       0.010000       \n",
            "10        1.195177       1.228774       0.010000       \n",
            "11        1.154600       1.220187       0.010000       \n",
            "12        1.115303       1.182996       0.010000       \n",
            "13        1.077430       1.198635       0.010000       \n",
            "14        1.042072       1.128678       0.010000       \n",
            "15        1.007538       1.116265       0.010000       \n",
            "16        0.974891       1.103808       0.010000       \n",
            "17        0.944346       1.078423       0.010000       \n",
            "18        0.913019       1.076948       0.010000       \n",
            "19        0.880419       1.059861       0.010000       \n",
            "20        0.850852       1.039128       0.010000       \n",
            "21        0.824350       1.098467       0.010000       \n",
            "22        0.796854       1.028167       0.010000       \n",
            "23        0.766172       1.053511       0.010000       \n",
            "24        0.739470       1.043088       0.010000       \n",
            "25        0.715823       1.001283       0.010000       \n",
            "26        0.685988       1.020872       0.010000       \n",
            "27        0.658652       1.057751       0.010000       \n",
            "28        0.633096       1.013974       0.010000       \n",
            "29        0.603365       1.036693       0.010000       \n",
            "30        0.575544       1.061274       0.010000       \n",
            "31        0.472464       1.006025       0.001000       \n",
            "32        0.455170       1.012301       0.001000       \n",
            "33        0.448318       1.018455       0.001000       \n",
            "34        0.442233       1.024635       0.001000       \n",
            "Early stopping at epoch 35\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=RMSProp, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         106.261533     1.930289       0.010000       \n",
            "2         1.998109       2.165634       0.010000       \n",
            "3         2.066675       1.996706       0.010000       \n",
            "4         2.017894       1.906616       0.010000       \n",
            "5         1.902781       1.815823       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=RMSProp, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         65.411283      2.186929       0.010000       \n",
            "2         2.201212       2.097366       0.010000       \n",
            "3         2.031611       1.916330       0.010000       \n",
            "4         1.964161       1.987299       0.010000       \n",
            "5         1.888073       1.893415       0.010000       \n",
            "6         1.838565       1.831940       0.010000       \n",
            "7         1.814664       1.785298       0.010000       \n",
            "8         1.768987       1.761746       0.010000       \n",
            "9         1.739171       1.731192       0.010000       \n",
            "10        1.674381       1.694895       0.010000       \n",
            "11        1.622564       1.600964       0.010000       \n",
            "12        1.602592       3.219802       0.010000       \n",
            "13        1.560389       1.636232       0.010000       \n",
            "14        1.535962       1.551908       0.010000       \n",
            "15        1.514680       1.514502       0.010000       \n",
            "16        1.517002       1.579672       0.010000       \n",
            "17        1.459873       1.631770       0.010000       \n",
            "18        1.411760       1.494037       0.010000       \n",
            "19        1.387825       1.641440       0.010000       \n",
            "20        1.370566       1.618237       0.010000       \n",
            "21        1.344835       1.461149       0.010000       \n",
            "22        1.315687       1.697460       0.010000       \n",
            "23        1.474787       1.468197       0.010000       \n",
            "24        1.291595       1.478354       0.010000       \n",
            "25        1.270421       1.456238       0.010000       \n",
            "26        1.241654       1.529741       0.010000       \n",
            "27        1.229754       1.549972       0.010000       \n",
            "28        1.209750       1.552217       0.010000       \n",
            "29        1.241367       1.663958       0.010000       \n",
            "30        1.211645       1.519200       0.010000       \n",
            "31        0.944440       1.452675       0.001000       \n",
            "32        0.874392       1.491162       0.001000       \n",
            "33        0.842022       1.523705       0.001000       \n",
            "34        0.819373       1.551939       0.001000       \n",
            "35        0.799551       1.573782       0.001000       \n",
            "36        0.783258       1.598560       0.001000       \n",
            "37        0.766818       1.624460       0.001000       \n",
            "38        0.753275       1.678385       0.001000       \n",
            "39        0.737630       1.707470       0.001000       \n",
            "40        0.724894       1.740499       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=RMSProp, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         49.601031      2.096444       0.010000       \n",
            "2         2.044286       1.968243       0.010000       \n",
            "3         1.940139       1.869185       0.010000       \n",
            "4         1.914981       1.871297       0.010000       \n",
            "5         1.874655       1.813934       0.010000       \n",
            "6         1.828109       1.757902       0.010000       \n",
            "7         1.737148       1.746860       0.010000       \n",
            "8         1.677918       1.649817       0.010000       \n",
            "9         1.637547       1.621557       0.010000       \n",
            "10        1.587775       1.627401       0.010000       \n",
            "11        1.546357       1.620456       0.010000       \n",
            "12        1.508177       1.539562       0.010000       \n",
            "13        1.546827       1.582555       0.010000       \n",
            "14        1.445865       1.525512       0.010000       \n",
            "15        1.414013       1.516789       0.010000       \n",
            "16        1.538505       1.787894       0.010000       \n",
            "17        1.383930       1.472910       0.010000       \n",
            "18        1.351902       1.676122       0.010000       \n",
            "19        1.322495       1.437683       0.010000       \n",
            "20        1.317959       1.532015       0.010000       \n",
            "21        1.277144       1.484260       0.010000       \n",
            "22        1.279528       1.557970       0.010000       \n",
            "23        1.259347       1.545202       0.010000       \n",
            "24        1.245637       1.457415       0.010000       \n",
            "25        1.228574       1.503218       0.010000       \n",
            "26        1.210510       1.507451       0.010000       \n",
            "27        1.190197       1.457182       0.010000       \n",
            "28        1.181923       1.479657       0.010000       \n",
            "Early stopping at epoch 29\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=RMSProp, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         57.783671      2.061979       0.010000       \n",
            "2         2.134167       2.110943       0.010000       \n",
            "3         2.029175       1.922517       0.010000       \n",
            "4         1.922150       1.832696       0.010000       \n",
            "5         1.841829       1.788872       0.010000       \n",
            "6         1.776201       1.727742       0.010000       \n",
            "7         1.742750       1.709766       0.010000       \n",
            "8         1.955029       1.674951       0.010000       \n",
            "9         1.677093       1.668402       0.010000       \n",
            "10        1.674418       1.658514       0.010000       \n",
            "11        1.612835       1.638594       0.010000       \n",
            "12        1.627946       1.630375       0.010000       \n",
            "13        1.566860       1.679229       0.010000       \n",
            "14        1.534266       1.585507       0.010000       \n",
            "15        1.499140       1.651202       0.010000       \n",
            "16        1.464099       1.535910       0.010000       \n",
            "17        1.447190       1.551314       0.010000       \n",
            "18        1.421559       1.568313       0.010000       \n",
            "19        1.387815       1.518841       0.010000       \n",
            "20        1.385693       1.539721       0.010000       \n",
            "21        1.349737       1.482812       0.010000       \n",
            "22        1.384733       1.532490       0.010000       \n",
            "23        1.328101       1.543197       0.010000       \n",
            "24        1.315078       1.796470       0.010000       \n",
            "25        1.288491       2.138408       0.010000       \n",
            "26        1.284342       1.554632       0.010000       \n",
            "27        1.316020       1.540491       0.010000       \n",
            "28        1.240866       1.549974       0.010000       \n",
            "29        1.257480       1.494274       0.010000       \n",
            "30        1.222885       1.516377       0.010000       \n",
            "31        0.964200       1.425256       0.001000       \n",
            "32        0.908258       1.448971       0.001000       \n",
            "33        0.881404       1.455826       0.001000       \n",
            "34        0.862022       1.442722       0.001000       \n",
            "35        0.842459       1.494434       0.001000       \n",
            "36        0.826920       1.506149       0.001000       \n",
            "37        0.812231       1.552897       0.001000       \n",
            "38        0.798104       1.555370       0.001000       \n",
            "39        0.785091       1.586591       0.001000       \n",
            "40        0.771817       1.635896       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=RMSProp, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         93.046290      1.875559       0.010000       \n",
            "2         1.907614       1.767929       0.010000       \n",
            "3         1.945556       1.782946       0.010000       \n",
            "4         1.889105       2.243798       0.010000       \n",
            "5         1.798885       1.824048       0.010000       \n",
            "6         1.796552       1.862065       0.010000       \n",
            "7         1.667105       1.619227       0.010000       \n",
            "8         1.607660       1.653822       0.010000       \n",
            "9         1.550839       1.608804       0.010000       \n",
            "10        1.524221       1.573984       0.010000       \n",
            "11        1.471937       1.577788       0.010000       \n",
            "12        1.440143       1.557532       0.010000       \n",
            "13        1.449993       1.607029       0.010000       \n",
            "14        1.517838       1.555457       0.010000       \n",
            "15        1.382228       1.581882       0.010000       \n",
            "16        1.378792       1.780532       0.010000       \n",
            "17        1.345684       1.603145       0.010000       \n",
            "18        1.346974       1.614860       0.010000       \n",
            "19        1.363652       1.830231       0.010000       \n",
            "20        1.328819       1.685655       0.010000       \n",
            "21        1.334291       1.672589       0.010000       \n",
            "22        1.264063       1.628967       0.010000       \n",
            "23        1.325454       1.605295       0.010000       \n",
            "Early stopping at epoch 24\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=Adam, Epochs=5\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.075478       1.945345       0.010000       \n",
            "2         1.871316       1.876112       0.010000       \n",
            "3         1.836281       1.796107       0.010000       \n",
            "4         1.790362       1.825344       0.010000       \n",
            "5         1.776552       1.769033       0.010000       \n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=Adam, Epochs=50\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         1.994447       1.815308       0.010000       \n",
            "2         1.775980       1.764562       0.010000       \n",
            "3         1.712025       1.721690       0.010000       \n",
            "4         1.663679       1.644038       0.010000       \n",
            "5         1.632252       1.651722       0.010000       \n",
            "6         1.590353       1.591280       0.010000       \n",
            "7         1.564550       1.623411       0.010000       \n",
            "8         1.537180       1.552833       0.010000       \n",
            "9         1.522255       1.592749       0.010000       \n",
            "10        1.511775       1.572296       0.010000       \n",
            "11        1.497606       1.549961       0.010000       \n",
            "12        1.498468       1.575763       0.010000       \n",
            "13        1.485574       1.598616       0.010000       \n",
            "14        1.473508       1.528264       0.010000       \n",
            "15        1.454777       1.537536       0.010000       \n",
            "16        1.439400       1.579726       0.010000       \n",
            "17        1.437758       1.521269       0.010000       \n",
            "18        1.424062       1.521295       0.010000       \n",
            "19        1.457152       1.747290       0.010000       \n",
            "20        1.605877       1.657010       0.010000       \n",
            "21        1.503101       1.557989       0.010000       \n",
            "22        1.418149       1.535416       0.010000       \n",
            "23        1.390645       1.590970       0.010000       \n",
            "24        1.371415       1.506698       0.010000       \n",
            "25        1.369542       1.550818       0.010000       \n",
            "26        1.348969       1.548356       0.010000       \n",
            "27        1.335818       1.506355       0.010000       \n",
            "28        1.317870       1.477842       0.010000       \n",
            "29        1.300250       1.501816       0.010000       \n",
            "30        1.282134       1.548393       0.010000       \n",
            "31        1.116963       1.446714       0.001000       \n",
            "32        1.065816       1.453557       0.001000       \n",
            "33        1.042342       1.466417       0.001000       \n",
            "34        1.027119       1.459400       0.001000       \n",
            "35        1.012375       1.490266       0.001000       \n",
            "36        0.999958       1.490967       0.001000       \n",
            "37        0.987572       1.510551       0.001000       \n",
            "38        0.977690       1.512082       0.001000       \n",
            "39        0.967552       1.522776       0.001000       \n",
            "40        0.958267       1.552889       0.001000       \n",
            "Early stopping at epoch 41\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=Adam, Epochs=100\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.326950       2.304514       0.010000       \n",
            "2         2.303724       2.304195       0.010000       \n",
            "3         2.303539       2.304068       0.010000       \n",
            "4         2.303596       2.304012       0.010000       \n",
            "5         2.303698       2.304483       0.010000       \n",
            "6         2.303553       2.303674       0.010000       \n",
            "7         2.303451       2.304302       0.010000       \n",
            "8         2.303518       2.303530       0.010000       \n",
            "9         2.303380       2.305135       0.010000       \n",
            "10        2.303677       2.303539       0.010000       \n",
            "11        2.303422       2.302887       0.010000       \n",
            "12        2.303538       2.302499       0.010000       \n",
            "13        2.303607       2.303508       0.010000       \n",
            "14        2.303585       2.304346       0.010000       \n",
            "15        2.303635       2.303321       0.010000       \n",
            "16        2.303550       2.303042       0.010000       \n",
            "17        2.303504       2.304134       0.010000       \n",
            "18        2.303781       2.303385       0.010000       \n",
            "19        2.303737       2.303472       0.010000       \n",
            "20        2.303500       2.303246       0.010000       \n",
            "21        2.303577       2.304103       0.010000       \n",
            "Early stopping at epoch 22\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=Adam, Epochs=250\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.325168       2.303911       0.010000       \n",
            "2         2.303652       2.303164       0.010000       \n",
            "3         2.303700       2.303267       0.010000       \n",
            "4         2.303526       2.303756       0.010000       \n",
            "5         2.303793       2.302658       0.010000       \n",
            "6         2.303516       2.304338       0.010000       \n",
            "7         2.303573       2.304040       0.010000       \n",
            "8         2.303469       2.304202       0.010000       \n",
            "9         2.303649       2.303991       0.010000       \n",
            "10        2.303506       2.305048       0.010000       \n",
            "11        2.303667       2.304150       0.010000       \n",
            "12        2.303541       2.304813       0.010000       \n",
            "13        2.303722       2.302911       0.010000       \n",
            "14        2.303633       2.303437       0.010000       \n",
            "Early stopping at epoch 15\n",
            "\n",
            "Running experiment: Kernel Size=7, Pooling=avg, Optimizer=Adam, Epochs=350\n",
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.328567       2.303474       0.010000       \n",
            "2         2.303615       2.305215       0.010000       \n",
            "3         2.303509       2.305667       0.010000       \n",
            "4         2.303775       2.303497       0.010000       \n",
            "5         2.303469       2.303266       0.010000       \n",
            "6         2.303501       2.303082       0.010000       \n",
            "7         2.303588       2.302798       0.010000       \n",
            "8         2.303547       2.303824       0.010000       \n",
            "9         2.303694       2.303298       0.010000       \n",
            "10        2.303648       2.303064       0.010000       \n",
            "11        2.303370       2.303580       0.010000       \n",
            "12        2.303769       2.304784       0.010000       \n",
            "13        2.303465       2.304093       0.010000       \n",
            "14        2.303479       2.303145       0.010000       \n",
            "15        2.303683       2.303993       0.010000       \n",
            "16        2.303640       2.304128       0.010000       \n",
            "Early stopping at epoch 17\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import pandas as pd\n",
        "\n",
        "# Hyperparameter setup\n",
        "kernel_sizes = [3, 5, 7]\n",
        "pooling_types = ['max', 'avg']\n",
        "optimizers = {'SGD': SGD, 'RMSProp': RMSprop, 'Adam': Adam}\n",
        "epochs_list = [5, 50, 100, 250, 350]\n",
        "\n",
        "results = []\n",
        "\n",
        "# Loop eksperimen\n",
        "for kernel_size, pooling, (opt_name, opt_fn), epochs in itertools.product(\n",
        "    kernel_sizes, pooling_types, optimizers.items(), epochs_list):\n",
        "    \n",
        "    print(f\"\\nRunning experiment: Kernel Size={kernel_size}, Pooling={pooling}, Optimizer={opt_name}, Epochs={epochs}\")\n",
        "    model = CNN(kernel_size=kernel_size, pooling=pooling)\n",
        "    optimizer = opt_fn(model.parameters(), lr=0.01)\n",
        "    best_val_loss = train_model(model, optimizer, epochs, early_stop_patience=10, lr_scheduler=True, device=device)\n",
        "    \n",
        "    # Log hasil\n",
        "    results.append({\n",
        "        'Kernel Size': kernel_size,\n",
        "        'Pooling': pooling,\n",
        "        'Optimizer': opt_name,\n",
        "        'Epochs': epochs,\n",
        "        'Best Val Loss': best_val_loss\n",
        "    })\n",
        "\n",
        "# Simpan hasil ke CSV\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv('experiment_results.csv', index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Visualize training history\n",
        "def plot_training_history(history, title=\"Training and Validation Loss\"):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(history['train_loss'], label='Train Loss', color='blue')\n",
        "    plt.plot(history['val_loss'], label='Validation Loss', color='orange')\n",
        "    plt.title(title, fontsize=16)\n",
        "    plt.xlabel('Epochs', fontsize=14)\n",
        "    plt.ylabel('Loss', fontsize=14)\n",
        "    plt.legend(fontsize=12)\n",
        "    plt.grid(alpha=0.3)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch     Train Loss     Val Loss       Learning Rate  \n",
            "==================================================\n",
            "1         2.303556       2.303743       0.010000       \n",
            "2         2.303693       2.302739       0.010000       \n",
            "3         2.303655       2.304926       0.010000       \n",
            "4         2.303425       2.303469       0.010000       \n",
            "5         2.303678       2.303280       0.010000       \n",
            "6         2.303512       2.303968       0.010000       \n",
            "7         2.303514       2.304083       0.010000       \n",
            "8         2.303421       2.304421       0.010000       \n",
            "9         2.303630       2.303165       0.010000       \n",
            "10        2.303485       2.302850       0.010000       \n",
            "11        2.303617       2.304052       0.010000       \n",
            "Early stopping at epoch 12\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Panggil fungsi train_model\n",
        "history = train_model(model, optimizer, epochs=50, early_stop_patience=10, lr_scheduler=True, device=device)\n",
        "\n",
        "# Plot hasilnya\n",
        "plot_training_history(history)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    Kernel Size Pooling Optimizer  Epochs  Best Val Accuracy\n",
            "34            5     max       SGD     350           0.115198\n",
            "61            7     max       SGD      50           0.114594\n",
            "31            5     max       SGD      50           0.107082\n",
            "32            5     max       SGD     100           0.104410\n",
            "64            7     max       SGD     350           0.099890\n",
            "62            7     max       SGD     100           0.097994\n",
            "33            5     max       SGD     250           0.093957\n",
            "63            7     max       SGD     250           0.090622\n",
            "3             3     max       SGD     250           0.078070\n",
            "1             3     max       SGD      50           0.072847\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import ast\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Lokasi file hasil eksperimen\n",
        "results_path = 'experiment_results.csv'\n",
        "\n",
        "# Baca file CSV\n",
        "df = pd.read_csv(results_path)\n",
        "\n",
        "# Tambahkan kolom 'Best Val Accuracy' berdasarkan 'Best Val Loss'\n",
        "def calculate_accuracy_from_loss(val_loss):\n",
        "    val_loss_dict = ast.literal_eval(val_loss)  # Konversi string menjadi dictionary\n",
        "    val_loss_list = val_loss_dict['val_loss']\n",
        "    best_loss = min(val_loss_list)\n",
        "    # Konversi loss ke akurasi (asumsi: akurasi = 1 - normalized_loss)\n",
        "    best_accuracy = 1 - best_loss\n",
        "    return best_accuracy\n",
        "\n",
        "df['Best Val Accuracy'] = df['Best Val Loss'].apply(calculate_accuracy_from_loss)\n",
        "\n",
        "# Urutkan berdasarkan akurasi terbaik\n",
        "top_10_results = df.sort_values(by='Best Val Accuracy', ascending=False).head(10)\n",
        "\n",
        "# Tampilkan DataFrame hasil\n",
        "print(top_10_results[['Kernel Size', 'Pooling', 'Optimizer', 'Epochs', 'Best Val Accuracy']])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Plot barplot\n",
        "plt.figure(figsize=(12, 6))\n",
        "bar_labels = [\n",
        "    f\"{row['Kernel Size']}|{row['Pooling']}|{row['Optimizer']}|{row['Epochs']} Epochs\"\n",
        "    for _, row in top_10_results.iterrows()\n",
        "]\n",
        "plt.bar(bar_labels, top_10_results['Best Val Accuracy'], color='skyblue')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
        "plt.xlabel('Experiment Settings', fontsize=14)\n",
        "plt.ylabel('Best Validation Accuracy', fontsize=14)\n",
        "plt.title('Top 10 Best Validation Accuracy Results', fontsize=16)\n",
        "plt.grid(alpha=0.3, linestyle='--', axis='y')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python3.9",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
